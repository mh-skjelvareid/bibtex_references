Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Chen2021,
abstract = {How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to ×30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths. Our project page with code is at https://yinboc.github.io/liif/.},
archivePrefix = {arXiv},
arxivId = {2012.09161},
author = {Chen, Yinbo and Liu, Sifei and Wang, Xiaolong},
doi = {10.1109/CVPR46437.2021.00852},
eprint = {2012.09161},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2021/Chen, Liu, Wang_2021_Learning Continuous Image Representation with Local Implicit Image Function.pdf:pdf},
isbn = {9781665445092},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8624--8634},
title = {{Learning Continuous Image Representation with Local Implicit Image Function}},
year = {2021}
}
@misc{Hennessy2020,
abstract = {Hyperspectral sensing, measuring reflectance over visible to shortwave infrared wavelengths, has enabled the classification and mapping of vegetation at a range of taxonomic scales, often down to the species level. Classification with hyperspectral measurements, acquired by narrow band spectroradiometers or imaging sensors, has generally required some form of spectral feature selection to reduce the dimensionality of the data to a level suitable for the construction of a classification model. Despite the large number of hyperspectral plant classification studies, an in-depth review of feature selection methods and resultant waveband selections has not yet been performed. Here, we present a review of the last 22 years of hyperspectral vegetation classification literature that evaluates the overall waveband selection frequency, waveband selection frequency variation by taxonomic, structural, or functional group, and the influence of feature selection choice by comparing such methods as stepwise discriminant analysis (SDA), support vector machines (SVM), and random forests (RF). This review determined that all characteristics of hyperspectral plant studies influence the wavebands selected for classification. This includes the taxonomic, structural, and functional groups of the target samples, the methods, and scale at which hyperspectral measurements are recorded, as well as the feature selection method used. Furthermore, these influences do not appear to be consistent. Moreover, the considerable variability in waveband selection caused by the feature selectors effectively masks the analysis of any variability between studies related to plant groupings. Additionally, questions are raised about the suitability of SDA as a feature selection method, with it producing waveband selections at odds with the other feature selectors. Caution is recommended when choosing a feature selector for hyperspectral plant classification: We recommend multiple methods being performed. The resultant sets of selected spectral features can either be evaluated individually by multiple classification models or combined as an ensemble for evaluation by a single classifier. Additionally, we suggest caution when relying upon waveband recommendations from the literature to guide waveband selections or classifications for new plant discrimination applications, as such recommendations appear to be weakly generalizable between studies.},
author = {Hennessy, Andrew and Clarke, Kenneth and Lewis, Megan},
booktitle = {Remote Sensing},
doi = {10.3390/RS12010113},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Hennessy, Clarke, Lewis_2020_Hyperspectral Classification of Plants A Review of Waveband Selection Generalisability.pdf:pdf},
issn = {20724292},
keywords = {Classification,Discrimination,Feature selection,Hyperspectral,Plant,Random forest,Spectra,Support vector machine,Vegetation,Waveband selection},
month = {jan},
number = {1},
pages = {113},
publisher = {MDPI AG},
title = {{Hyperspectral classification of plants: A review of waveband selection generalisability}},
url = {https://www.mdpi.com/2072-4292/12/1/113},
volume = {12},
year = {2020}
}
@inproceedings{Christie2017,
abstract = {We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a 'false detection' category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1711.07846},
author = {Christie, Gordon and Fendley, Neil and Wilson, James and Mukherjee, Ryan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00646},
eprint = {1711.07846},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Christie et al._2017_Functional Map of the World.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
month = {nov},
pages = {6172--6180},
title = {{Functional Map of the World}},
url = {http://arxiv.org/abs/1711.07846},
year = {2018}
}
@misc{Kiran2018,
abstract = {Videos represent the primary source of information for surveillance applications. Video material is often available in large quantities but in most cases it contains little or no annotation for supervised learning. This article reviews the state-of-the-art deep learning based methods for video anomaly detection and categorizes them based on the type of model and criteria of detection. We also perform simple studies to understand the different approaches and provide the criteria of evaluation for spatio-temporal anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1801.03149},
author = {Kiran, B. Ravi and Thomas, Dilip Mathew and Parakkal, Ranjith},
booktitle = {Journal of Imaging},
doi = {10.3390/jimaging4020036},
eprint = {1801.03149},
issn = {2313433X},
keywords = {Anomaly detection,Autoencoders,Generative adversarial networks,LSTMs,Predictive models,Representation learning,Unsupervised methods,Variational Autoencoders},
number = {2},
publisher = {MDPI Multidisciplinary Digital Publishing Institute},
title = {{An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos}},
volume = {4},
year = {2018}
}
@article{Li2022,
abstract = {Humans are able to recognize structured relations in observation, allowing us to decompose complex scenes into simpler parts and abstract the visual world in multiple levels. However, such hierarchical reasoning ability of human perception remains largely unexplored in current literature of semantic segmentation. Existing work is often aware of flatten labels and predicts target classes exclusively for each pixel. In this paper, we instead address hierarchical semantic segmentation (HSS), which aims at structured, pixel-wise description of visual observation in terms of a class hierarchy. We devise HSSN, a general HSS framework that tackles two critical issues in this task: i) how to efficiently adapt existing hierarchy-agnostic segmentation networks to the HSS setting, and ii) how to leverage the hierarchy information to regularize HSS network learning. To address i), HSSN directly casts HSS as a pixel-wise multi-label classification task, only bringing minimal architecture change to current segmentation models. To solve ii), HSSN first explores inherent properties of the hierarchy as a training objective, which enforces segmentation predictions to obey the hierarchy structure. Further, with hierarchy-induced margin constraints, HSSNreshapes the pixel embedding space, so as to generate well-structured pixel representations and improve segmentation eventually. We conduct experiments on four semantic segmentation datasets (i.e., Mapillary Vistas 2.0, City-scapes, LIP, and PASCAL-Person-Part), with different class hierarchies, segmentation network architectures and backbones, showing the generalization and superiority of HSSN.},
archivePrefix = {arXiv},
arxivId = {2203.14335},
author = {Li, Liulei and Zhou, Tianfei and Wang, Wenguan and Li, Jianwu and Yang, Yi},
doi = {10.1109/CVPR52688.2022.00131},
eprint = {2203.14335},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2022/Li et al._2022_Deep Hierarchical Semantic Segmentation.pdf:pdf},
isbn = {9781665469463},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Segmentation,grouping and shape analysis},
pages = {1236--1247},
title = {{Deep Hierarchical Semantic Segmentation}},
volume = {2022-June},
year = {2022}
}
@article{Traganos2018,
abstract = {In the epoch of the human-induced climate change, seagrasses can mitigate the resulting negative impacts due to their carbon sequestration ability. The endemic and dominant in the Mediterranean Posidonia oceanica seagrass contains the largest stocks of organic carbon among all seagrass species, yet it undergoes a significant regression in its extent. Therefore, suitable quantitative assessment of its extent and optically shallow environment are required to allow good conservation and management practices. Here, we parameterise a semi-analytical inversion model which employs above-surface remote sensing reflectance of Sentinel-2A to derive water column and bottom properties in the Thermaikos Gulf, NW Aegean Sea, Greece (eastern Mediterranean). In the model, the diffuse attenuation coefficients are expressed as functions of absorption and backscattering coefficients. We apply a comprehensive pre-processing workflow which includes atmospheric correction using C2RCC (Case 2 Regional CoastColour) neural network, resampling of the lower spatial resolution Sentinel-2A bands to 10m/pixel, as well as empirical derivation of water bathymetry and machine learning-based classification of the resulting bottom properties using the Support Vector Machines. SVM-based classification of benthic reflectance reveals $\sim$300 ha of P. oceanica seagrass between 2 and 16 m of depth, and yields very high producer and user accuracies of 95.3% and 99.5%, respectively. Sources of errors and uncertainties are discussed. All in all, recent advances in Earth Observation in terms of optical satellite technology, cloud computing and machine learning algorithms have created the perfect storm which could aid high spatio-temporal, large-scale seagrass habitat mapping and monitoring, allowing for its integration to the Analysis Ready Data era and ultimately enabling more efficient management and conservation in the epoch of climate change.},
author = {Traganos, Dimosthenis and Reinartz, Peter},
doi = {10.1080/01431161.2018.1519289},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Traganos, Reinartz_2018_Machine learning-based retrieval of benthic reflectance and Posidonia oceanica seagrass extent using a semi-anal.pdf:pdf},
issn = {13665901},
journal = {International Journal of Remote Sensing},
number = {24},
pages = {9428--9452},
publisher = {Taylor & Francis},
title = {{Machine learning-based retrieval of benthic reflectance and Posidonia oceanica seagrass extent using a semi-analytical inversion of Sentinel-2 satellite data}},
url = {https://doi.org/10.1080/01431161.2018.1519289},
volume = {39},
year = {2018}
}
@article{Thompson2017,
abstract = {Remote imaging spectroscopy from 400 to 800 nm can use benthic reflectance signatures to map the composition and condition of shallow water ecosystems. We present a novel probabilistic approach to jointly estimate the seafloor reflectance and water properties while flexibly incorporating varied domain knowledge and in situ measurements. The inversion transforms remote radiance data with an atmospheric correction followed by a water column correction. Benthic reflectance and water optical properties are both represented by linear mixtures of endmember spectra. We combine remote measurements, prior knowledge and field data using a flexible Bayesian optimal estimation, solving for the Maximum A Posteriori (MAP) combination of water column properties, seafloor reflectance, and depth. We then demonstrate performance in controlled simulations and in overflights of a coral reef in Hawaii with coincident in situ measurements. The measurement approach helps lay a foundation for wide-area airborne mapping of the condition of threatened coastal ecosystems such as coral reefs.},
author = {Thompson, David R. and Hochberg, Eric J. and Asner, Gregory P. and Green, Robert O. and Knapp, David E. and Gao, Bo Cai and Garcia, Rodrigo and Gierach, Michelle and Lee, Zhongping and Maritorena, Stephane and Fick, Ronald},
doi = {10.1016/j.rse.2017.07.030},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Atmospheric correction,Coral reefs,Imaging spectroscopy,Remote sensing},
month = {oct},
pages = {18--30},
publisher = {Elsevier Inc.},
title = {{Airborne mapping of benthic reflectance spectra with Bayesian linear mixtures}},
volume = {200},
year = {2017}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P},
doi = {10.1109/TIP.2003.819861},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2004/Wang et al._2004_Image Quality Assessment From Error Visibility to Structural Similarity.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
number = {4},
pages = {600--612},
pmid = {15376593},
title = {{Image quality assessment: From error visibility to structural similarity}},
url = {http://www.cns.nyu.edu/$\sim$lcv/ssim/.},
volume = {13},
year = {2004}
}
@article{Benz2004,
abstract = {Remote sensing from airborne and spaceborne platforms provides valuable data for mapping, environmental monitoring, disaster management and civil and military intelligence. However, to explore the full value of these data, the appropriate information has to be extracted and presented in standard format to import it into geo-information systems and thus allow efficient decision processes. The object-oriented approach can contribute to powerful automatic and semi-automatic analysis for most remote sensing applications. Synergetic use to pixel-based or statistical signal processing methods explores the rich information contents. Here, we explain principal strategies of object-oriented analysis, discuss how the combination with fuzzy methods allows implementing expert knowledge and describe a representative example for the proposed workflow from remote sensing imagery to GIS. The strategies are demonstrated using the first object-oriented image analysis software on the market, eCognition, which provides an appropriate link between remote sensing imagery and GIS. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Benz, Ursula C. and Hofmann, Peter and Willhauck, Gregor and Lingenfelder, Iris and Heynen, Markus},
doi = {10.1016/j.isprsjprs.2003.10.002},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2004/Benz et al._2004_Multi-resolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Fuzzy classification,GIS,Multi-resolution segmentation,Object-oriented image analysis,Remote sensing},
number = {3-4},
pages = {239--258},
title = {{Multi-resolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information}},
volume = {58},
year = {2004}
}
@misc{Pimentel2014,
abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as "one-class classification", in which a model is constructed to describe "normal" training data. The novelty detection approach is typically used when the quantity of available "abnormal" data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that "normality" may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade. {\textcopyright} 2014 Published by Elsevier B.V.},
author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
booktitle = {Signal Processing},
doi = {10.1016/j.sigpro.2013.12.026},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2014/Pimentel et al._2014_A review of novelty detection.pdf:pdf},
issn = {01651684},
keywords = {Machine learning,Novelty detection,One-class classification},
pages = {215--249},
title = {{A review of novelty detection}},
url = {http://dx.doi.org/10.1016/j.sigpro.2013.12.026},
volume = {99},
year = {2014}
}
@article{Hu2019,
abstract = {Detecting and locating surface defects in textured materials is a crucial but challenging problem due to factors such as texture variations and lack of adequate defective samples prior to testing. In this paper we present a novel unsupervised method for automatically detecting defects in fabrics based on a deep convolutional generative adversarial network (DCGAN). The proposed method extends the standard DCGAN, which consists of a discriminator and a generator, by introducing a new encoder component. With the assistance of this encoder, our model can reconstruct a given query image such that no defects but only normal textures will be preserved in the reconstruction. Therefore, when subtracting the reconstruction from the original image, a residual map can be created to highlight potential defective regions. Besides, our model generates a likelihood map for the image under inspection where each pixel value indicates the probability of occurrence of defects at that location. The residual map and the likelihood map are then synthesized together to form an enhanced fusion map. Typically, the fusion map exhibits uniform gray levels over defect-free regions but distinct deviations over defective areas, which can be further thresholded to produce a binarized segmentation result. Our model can be unsupervisedly trained by feeding with a set of small-sized image patches picked from a few defect-free examples. The training is divided into several successively performed stages, each under an individual training strategy. The performance of the proposed method has been extensively evaluated by a variety of real fabric samples. The experimental results in comparison with other methods demonstrate its effectiveness in fabric defect detection.},
author = {Hu, Guanghua and Huang, Junfeng and Wang, Qinghui and Li, Jingrong and Xu, Zhijia and Huang, Xingbiao},
doi = {10.1177/0040517519862880},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Hu et al._2019_Unsupervised fabric defect detection based on a deep convolutional generative adversarial network.pdf:pdf},
issn = {17467748},
journal = {Textile Research Journal},
keywords = {deep learning,defect detection,fabric inspection,generative adversarial network},
month = {jul},
number = {3-4},
pages = {247--270},
title = {{Unsupervised fabric defect detection based on a deep convolutional generative adversarial network}},
url = {http://journals.sagepub.com/doi/10.1177/0040517519862880},
volume = {90},
year = {2020}
}
@article{Silla2011,
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages. {\textcopyright} 2010 The Author(s).},
author = {Silla, Carlos N. and Freitas, Alex A.},
doi = {10.1007/s10618-010-0175-9},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2011/Silla, Freitas_2011_A survey of hierarchical classification across different application domains(2).pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {DAG-structured class hierarchies,Hierarchical classification,Tree-structured class hierarchies},
number = {1-2},
pages = {31--72},
title = {{A survey of hierarchical classification across different application domains}},
volume = {22},
year = {2011}
}
@article{McInnes2018,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John and Melville, James},
eprint = {1802.03426},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/McInnes, Healy, Melville_2018_UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf:pdf},
month = {feb},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {https://arxiv.org/abs/1802.03426v3 http://arxiv.org/abs/1802.03426},
year = {2018}
}
@inproceedings{Long2015a,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298965},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Long, Shelhamer, Darrell_2015_Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {431--440},
title = {{Fully convolutional networks for semantic segmentation}},
url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
volume = {07-12-June},
year = {2015}
}
@article{Wei2019,
abstract = {The fine classification of crops is critical for food security and agricultural management. There are many different species of crops, some of which have similar spectral curves. As a result, the precise classification of crops is a difficult task. Although the classification methods that incorporate spatial information can reduce the noise and improve the classification accuracy, to a certain extent, the problem is far from solved. Therefore, in this paper, the method of spatial-spectral fusion based on conditional random fields (SSF-CRF) for the fine classification of crops in UAV-borne hyperspectral remote sensing imagery is presented. The proposed method designs suitable potential functions in a pairwise conditional random field model, fusing the spectral and spatial features to reduce the spectral variation within the homogenous regions and accurately identify the crops. The experiments on hyperspectral datasets of the cities of Hanchuan and Honghu in China showed that, compared with the traditional methods, the proposed classification method can effectively improve the classification accuracy, protect the edges and shapes of the features, and relieve excessive smoothing, while retaining detailed information. This method has important significance for the fine classification of crops in hyperspectral remote sensing imagery.},
author = {Wei, Lifei and Yu, Ming and Zhong, Yanfei and Zhao, Ji and Liang, Yajing and Hu, Xin},
doi = {10.3390/rs11070780},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Wei et al._2019_Spatial-spectral fusion based on conditional random fields for the fine classification of crops in UAV-borne hyperspectr.pdf:pdf},
isbn = {2017111108110},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Conditional random fields,Fine crop classification,Hyperspectral remote sensing imagery,Spectral-spatial fusion,Unmanned aerial vehicle},
number = {7},
title = {{Spatial-spectral fusion based on conditional random fields for the fine classification of crops in UAV-borne hyperspectral remote sensing imagery}},
volume = {11},
year = {2019}
}
@article{Napoletano2018,
abstract = {Automatic detection and localization of anomalies in nanofibrous materials help to reduce the cost of the production process and the time of the post-production visual inspection process. Amongst all the monitoring methods, those exploiting Scanning Electron Microscope (SEM) imaging are the most effective. In this paper, we propose a region-based method for the detection and localization of anomalies in SEM images, based on Convolutional Neural Networks (CNNs) and self-similarity. The method evaluates the degree of abnormality of each subregion of an image under consideration by computing a CNN-based visual similarity with respect to a dictionary of anomaly-free subregions belonging to a training set. The proposed method outperforms the state of the art.},
author = {Napoletano, Paolo and Piccoli, Flavio and Schettini, Raimondo},
doi = {10.3390/s18010209},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Napoletano et al._2018_Anomaly Detection in Nanofibrous Materials by CNN-Based Self-Similarity.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Anomaly detection,Convolutional neural networks,Defect detection,Industrial quality inspection,Nanofibrous materials,Quality control},
month = {jan},
number = {1},
pages = {209},
pmid = {29329268},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Anomaly detection in nanofibrous materials by CNN-based self-similarity}},
url = {http://www.mdpi.com/1424-8220/18/1/209},
volume = {18},
year = {2018}
}
@article{Mundhenk,
abstract = {3D action recognition – analysis of human actions based on 3D skeleton data – becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains con-currently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.07043v1},
author = {Betina, V.},
doi = {10.1007/978-3-319-46487-9},
eprint = {arXiv:1607.07043v1},
file = {:C\:/Users/mha114/Dropbox/Litteratur/Unknown/Mundhenk et al._Unknown_A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning.pdf:pdf},
isbn = {9783319464879},
issn = {00282685},
journal = {Neoplasma},
keywords = {colorization},
number = {1},
pages = {23--32},
pmid = {5815023},
title = {{(Improved PPF)Going Further with Point Pair Features}},
url = {http://gdo-datasci.ucllnl.org/cowc/ http://link.springer.com/10.1007/978-3-319-46487-9},
volume = {16},
year = {2016}
}
@article{Zhao2015,
abstract = {We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.},
archivePrefix = {arXiv},
arxivId = {1506.02351},
author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and LeCun, Yann},
eprint = {1506.02351},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Zhao et al._2015_Stacked What-Where Auto-encoders.pdf:pdf},
title = {{Stacked What-Where Auto-encoders}},
url = {http://arxiv.org/abs/1506.02351},
year = {2015}
}
@article{Martin2021,
abstract = {Beach litter assessments rely on time inefficient and high human cost protocols, mining the attainment of global beach litter estimates. Here we show the application of an emerging technique, the use of drones for acquisition of high-resolution beach images coupled with machine learning for their automatic processing, aimed at achieving the first national-scale beach litter survey completed by only one operator. The aerial survey had a time efficiency of 570 ± 40 m2 min−1 and the machine learning reached a mean (±SE) detection sensitivity of 59 ± 3% with high resolution images. The resulting mean (±SE) litter density on Saudi Arabian shores of the Red Sea is of 0.12 ± 0.02 litter items m−2, distributed independently of the population density in the area around the sampling station. Instead, accumulation of litter depended on the exposure of the beach to the prevailing wind and litter composition differed between islands and the main shore, where recreational activities are the major source of anthropogenic debris. A national-scale monitoring of beach litter along the Red Sea coast of Saudi Arabia, conducted by a single drone operator, shows that litter distributes according to wind exposure.},
author = {Martin, Cecilia and Zhang, Qiannan and Zhai, Dongjun and Zhang, Xiangliang and Duarte, Carlos M.},
doi = {10.1016/j.envpol.2021.116730},
issn = {18736424},
journal = {Environmental Pollution},
keywords = {Beach litter,Deep neural network,Marine debris,Plastic,Unmanned aerial vehicles},
month = {feb},
pages = {116730},
pmid = {33652184},
publisher = {Elsevier},
title = {{Enabling a large-scale assessment of litter along Saudi Arabian red sea shores by combining drones and machine learning}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0269749121003109},
volume = {277},
year = {2021}
}
@article{Rolnick2017,
abstract = {Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.},
archivePrefix = {arXiv},
arxivId = {1705.10694},
author = {Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
eprint = {1705.10694},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Rolnick et al._2017_Deep Learning is Robust to Massive Label Noise.pdf:pdf},
title = {{Deep Learning is Robust to Massive Label Noise}},
url = {http://arxiv.org/abs/1705.10694},
year = {2017}
}
@article{Langenkamper2017,
abstract = {Combining state-of-the art digital imaging technology with different kinds of marine exploration techniques such as modern autonomous underwater vehicle (AUV), remote operating vehicle (ROV) or other monitoring platforms enables marine imaging on new spatial and/or temporal scales. A comprehensive interpretation of such image collections requires the detection, classification and quantification of objects of interest (OOI) in the images usually performed by domain experts. However, the data volume and the rich content of the images makes the support by software tools inevitable. We define some requirements for marine image annotation and present our new online tool BIIGLE 2.0. It is developed with a special focus on annotating benthic fauna in marine image collections with tools customized to increase efficiency and effectiveness in the manual annotation process. The software architecture of the system is described and the special features of BIIGLE 2.0 are illustrated with different use-cases and future developments are discussed.},
author = {Langenk{\"{a}}mper, Daniel and Zurowietz, Martin and Schoening, Timm and Nattkemper, Tim W.},
doi = {10.3389/fmars.2017.00083},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Langenk{\"{a}}mper et al._2017_BIIGLE 2.0 - browsing and annotating large marine image collections.pdf:pdf},
issn = {22967745},
journal = {Frontiers in Marine Science},
keywords = {Data bases,Environmental sciences,Human computer interaction (HCI),Image annotation,Marine biology,Marine imaging,Megafauna,Underwater image analysis system},
month = {mar},
number = {MAR},
pages = {254291},
publisher = {Frontiers Media S. A},
title = {{BIIGLE 2.0 - browsing and annotating large marine image collections}},
volume = {4},
year = {2017}
}
@article{Li2017,
abstract = {Recent research has shown that using spectral-spatial information can considerably improve the performance of hyperspectral image (HSI) classification. HSI data is typically presented in the format of 3D cubes. Thus, 3D spatial filtering naturally offers a simple and effective method for simultaneously extracting the spectral-spatial features within such images. In this paper, a 3D convolutional neural network (3D-CNN) framework is proposed for accurate HSI classification. The proposed method views the HSI cube data altogether without relying on any preprocessing or post-processing, extracting the deep spectral-spatial-combined features effectively. In addition, it requires fewer parameters than other deep learning-based methods. Thus, the model is lighter, less likely to over-fit, and easier to train. For comparison and validation, we test the proposed method along with three other deep learning-based HSI classification methods-namely, stacked autoencoder (SAE), deep brief network (DBN), and 2D-CNN-based methods-on three real-world HSI datasets captured by different sensors. Experimental results demonstrate that our 3D-CNN-based method outperforms these state-of-the-art methods and sets a new record.},
author = {Li, Ying and Zhang, Haokui and Shen, Qiang},
doi = {10.3390/rs9010067},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Li, Zhang, Shen_2017_Spectral–Spatial Classification of Hyperspectral Imagery with 3D Convolutional Neural Network.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {2D convolutional neural networks,3D convolutional neural networks,3D structure,Deep learning,Hyperspectral image classification},
month = {jan},
number = {1},
pages = {67},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Spectral-spatial classification of hyperspectral imagery with 3D convolutional neural network}},
url = {https://www.mdpi.com/2072-4292/9/1/67/htm https://www.mdpi.com/2072-4292/9/1/67},
volume = {9},
year = {2017}
}
@article{Kemker2017,
abstract = {In this paper, we study self-taught learning for hyperspectral image (HSI) classification. Supervised deep learning methods are currently state of the art for many machine learning problems, but these methods require large quantities of labeled data to be effective. Unfortunately, existing labeled HSI benchmarks are too small to directly train a deep supervised network. Alternatively, we used self-taught learning, which is an unsupervised method to learn feature extracting frameworks from unlabeled hyperspectral imagery. These models learn how to extract generalizable features by training on sufficiently large quantities of unlabeled data that are distinct from the target data set. Once trained, these models can extract features from smaller labeled target data sets. We studied two self-taught learning frameworks for HSI classification. The first is a shallow approach that uses independent component analysis and the second is a three-layer stacked convolutional autoencoder. Our models are applied to the Indian Pines, Salinas Valley, and Pavia University data sets, which were captured by two separate sensors at different altitudes. Despite large variation in scene type, our algorithms achieve state-of-the-art results across all the three data sets.},
author = {Kemker, Ronald and Kanan, Christopher},
doi = {10.1109/TGRS.2017.2651639},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Kemker, Kanan_2017_Self-Taught Feature Learning for Hyperspectral Image Classification.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Autoencoder,deep learning,feature learning,hyperspectral imaging,independent component analysis (ICA),self-taught learning},
number = {5},
pages = {2693--2705},
publisher = {IEEE},
title = {{Self-Taught Feature Learning for Hyperspectral Image Classification}},
volume = {55},
year = {2017}
}
@article{Sadr2019,
abstract = {Anomaly detection is challenging, especially for large datasets in high dimensions. Here we explore a general anomaly detection framework based on dimensionality reduction and unsupervised clustering. We release DRAMA, a general python package that implements the general framework with a wide range of built-in options. We test DRAMA on a wide variety of simulated and real datasets, in up to 3000 dimensions, and find it robust and highly competitive with commonly-used anomaly detection algorithms, especially in high dimensions. The flexibility of the DRAMA framework allows for significant optimization once some examples of anomalies are available, making it ideal for online anomaly detection, active learning and highly unbalanced datasets.},
archivePrefix = {arXiv},
arxivId = {1909.04060},
author = {Sadr, Alireza Vafaei and Bassett, Bruce A. and Kunz, Martin},
eprint = {1909.04060},
title = {{A Flexible Framework for Anomaly Detection via Dimensionality Reduction}},
url = {http://arxiv.org/abs/1909.04060},
year = {2019}
}
@inproceedings{Chen2017,
abstract = {In this paper, we introduce autoencoder ensembles for unsupervised outlier detection. One problem with neural networks is that they are sensitive to noise and often require large data sets to work robustly, while increasing data size makes them slow. As a result, there are only a few existing works in the literature on the use of neural networks in outlier detection. This paper shows that neural networks can be a very competitive technique to other existing methods. The basic idea is to randomly vary on the connectivity architecture of the autoencoder to obtain significantly better performance. Furthermore, we combine this technique with an adaptive sampling method to make our approach more efficient and effective. Experimental results comparing the proposed approach with state-of-the- art detectors are presented on several benchmark data sets showing the accuracy of our approach.},
author = {Chen, Jinghui and Sathe, Saket and Aggarwal, Charu and Turaga, Deepak},
booktitle = {Proceedings of the 17th SIAM International Conference on Data Mining, SDM 2017},
doi = {10.1137/1.9781611974973.11},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Chen et al._2017_Outlier detection with autoencoder ensembles.pdf:pdf},
isbn = {9781611974874},
pages = {90--98},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{Outlier detection with autoencoder ensembles}},
year = {2017}
}
@article{Liu2019,
abstract = {Texture is a fundamental characteristic of many types of images, and texture representation is one of the essential and challenging problems in computer vision and pattern recognition which has attracted extensive research attention over several decades. Since 2000, texture representations based on Bag of Words and on Convolutional Neural Networks have been extensively studied with impressive performance. Given this period of remarkable evolution, this paper aims to present a comprehensive survey of advances in texture representation over the last two decades. More than 250 major publications are cited in this survey covering different aspects of the research, including benchmark datasets and state of the art results. In retrospect of what has been achieved so far, the survey discusses open challenges and directions for future research.},
archivePrefix = {arXiv},
arxivId = {1801.10324},
author = {Liu, Li and Chen, Jie and Fieguth, Paul and Zhao, Guoying and Chellappa, Rama and Pietik{\"{a}}inen, Matti},
doi = {10.1007/s11263-018-1125-z},
eprint = {1801.10324},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Liu et al._2019_From BoW to CNN Two Decades of Texture Representation for Texture Classification.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Bag of Words,Computer vision,Convolutional Neural Network,Deep learning,Feature extraction,Local descriptors,Texture classification,Visual attributes},
number = {1},
pages = {74--109},
title = {{From BoW to CNN: Two Decades of Texture Representation for Texture Classification}},
volume = {127},
year = {2019}
}
@article{Trier2018,
abstract = {This article compares four new automatic methods to discriminate between spruce, pine and birch, which are the dominating tree species in Norwegian forests. Airborne laser scanning and hyperspectral data were used. The laser scanning data was used to mask pixels with low or no vegetation in the hyperspectral data. A green–blue ratio was used to remove shadow areas from tree canopies, and the normalized difference vegetation index to remove dead vegetation and non-vegetation. The best method was hyperspectral pixel classification with 160 spectral channels in the visible and near-infrared spectrum, using a deep neural network. This method achieved 87% correct classification rate. Partial least squares regression for hyperspectral pixel classification achieved 78%. Deep neural network image classification using canopy height blended with three hyperspectral channels achieved 74%. A simple pixel classification method based on two spectral indices resulted in 67% correct classification. A possible future improvement is to find a better way to combine hyperspectral data with canopy height data in a deep neural network.},
author = {Trier, {\O}ivind Due and Salberg, Arnt B{\o}rre and Kermit, Martin and Rudjord, {\O}ystein and Gobakken, Terje and N{\ae}sset, Erik and Aarsten, Dagrun},
doi = {10.1080/22797254.2018.1434424},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Trier et al._2018_Tree species classification in Norway from airborne hyperspectral and airborne laser scanning data.pdf:pdf},
issn = {22797254},
journal = {European Journal of Remote Sensing},
keywords = {Lidar,automatic processing,canopy height model,deep learning,forestry,imaging spectroscopy},
month = {jan},
number = {1},
pages = {336--351},
publisher = {Taylor & Francis},
title = {{Tree species classification in Norway from airborne hyperspectral and airborne laser scanning data}},
url = {https://www.tandfonline.com/doi/abs/10.1080/22797254.2018.1434424},
volume = {51},
year = {2018}
}
@article{VanEtten2018,
abstract = {Foundational mapping remains a challenge in many parts of the world, particularly in dynamic scenarios such as natural disasters when timely updates are critical. Updating maps is currently a highly manual process requiring a large number of human labelers to either create features or rigorously validate automated outputs. We propose that the frequent revisits of earth imaging satellite constellations may accelerate existing efforts to quickly update foundational maps when combined with advanced machine learning techniques. Accordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and NVIDIA), released a large corpus of labeled satellite imagery on Amazon Web Services (AWS) called SpaceNet. The SpaceNet partners also launched a series of public prize competitions to encourage improvement of remote sensing machine learning algorithms. The first two of these competitions focused on automated building footprint extraction, and the most recent challenge focused on road network extraction. In this paper we discuss the SpaceNet imagery, labels, evaluation metrics, prize challenge results to date, and future plans for the SpaceNet challenge series.},
archivePrefix = {arXiv},
arxivId = {1807.01232},
author = {{Van Etten}, Adam and Lindenbaum, Dave and Bacastow, Todd M.},
eprint = {1807.01232},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Van Etten, Lindenbaum, Bacastow_2018_SpaceNet A Remote Sensing Dataset and Challenge Series.pdf:pdf},
month = {jul},
title = {{SpaceNet: A Remote Sensing Dataset and Challenge Series}},
url = {http://arxiv.org/abs/1807.01232},
year = {2018}
}
@article{Parsons2018b,
abstract = {Recent advances in unmanned aerial system (UAS) sensed imagery, sensor quality/size, and geospatial image processing can enable UASs to rapidly and continually monitor coral reefs, to determine the type of coral and signs of coral bleaching. This paper describes an unmanned aerial vehicle (UAV) remote sensing methodology to increase the efficiency and accuracy of existing surveillance practices. The methodology uses a UAV integrated with advanced digital hyperspectral, ultra HD colour (RGB) sensors, and machine learning algorithms. This paper describes the combination of airborne RGB and hyperspectral imagery with in-water survey data of several types in-water survey of coral under diverse levels of bleaching. The paper also describes the technology used, the sensors, the UAS, the flight operations, the processing workflow of the datasets, the methods for combining multiple airborne and in-water datasets, and finally presents relevant results of material classification. The development of the methodology for the collection and analysis of airborne hyperspectral and RGB imagery would provide coral reef researchers, other scientists, and UAV practitioners with reliable data collection protocols and faster processing techniques to achieve remote sensing objectives.},
author = {Parsons, Mark and Bratanov, Dmitry and Gaston, Kevin J and Gonzalez, Felipe},
doi = {10.3390/s18072026},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Parsons et al._2018_UAVs, Hyperspectral Remote Sensing, and Machine Learning Revolutionizing Reef Monitoring.pdf:pdf},
journal = {Sensors},
keywords = {UAS,drones,hyperspectral camera,image segmentation,in-water survey,machine learning,support vector machines (SVM)},
number = {7},
pages = {2026},
title = {{UAVs, Hyperspectral Remote Sensing, and Machine Learning Revolutionizing Reef Monitoring}},
url = {www.mdpi.com/journal/sensors},
volume = {18},
year = {2018}
}
@article{Weimer2016,
abstract = {Fast and reliable industrial inspection is a main challenge in manufacturing scenarios. However, the defect detection performance is heavily dependent on manually defined features for defect representation. In this contribution, we investigate a new paradigm from machine learning, namely deep machine learning by examining design configurations of deep Convolutional Neural Networks (CNN) and the impact of different hyper-parameter settings towards the accuracy of defect detection results. In contrast to manually designed image processing solutions, deep CNN automatically generate powerful features by hierarchical learning strategies from massive amounts of training data with a minimum of human interaction or expert process knowledge. An application of the proposed method demonstrates excellent defect detection results with low false alarm rates.},
author = {Weimer, Daniel and Scholz-Reiter, Bernd and Shpitalni, Moshe},
doi = {10.1016/j.cirp.2016.04.072},
issn = {17260604},
journal = {CIRP Annals - Manufacturing Technology},
keywords = {Artificial intelligence,Deep machine learning,Quality assurance},
month = {jan},
number = {1},
pages = {417--420},
publisher = {Elsevier},
title = {{Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection}},
url = {https://www.sciencedirect.com/science/article/pii/S0007850616300725},
volume = {65},
year = {2016}
}
@article{Masarczyk2020,
abstract = {Hyperspectral imaging is a rich source of data, allowing for a multitude of effective applications. However, such imaging remains challenging because of large data dimension and, typically, a small pool of available training examples. While deep learning approaches have been shown to be successful in providing effective classification solutions, especially for high dimensional problems, unfortunately they work best with a lot of labelled examples available. The transfer learning approach can be used to alleviate the second requirement for a particular dataset: first the network is pre-trained on some dataset with large amount of training labels available, then the actual dataset is used to fine-tune the network. This strategy is not straightforward to apply with hyperspectral images, as it is often the case that only one particular image of some type or characteristic is available. In this paper, we propose and investigate a simple and effective strategy of transfer learning that uses unsupervised pre-training step without label information. This approach can be applied to many of the hyperspectral classification problems. The performed experiments show that it is very effective at improving the classification accuracy without being restricted to a particular image type or neural network architecture. The experiments were carried out on several deep neural network architectures and various sizes of labeled training sets. The greatest improvement in overall accuracy on the Indian Pines and Pavia University datasets is over 21 and 13 percentage points, respectively. An additional advantage of the proposed approach is the unsupervised nature of the pre-training step, which can be done immediately after image acquisition, without the need of the potentially costly expert's time.},
archivePrefix = {arXiv},
arxivId = {1909.05507},
author = {Masarczyk, Wojciech and Glomb, Przemyslaw and Grabowski, Bartosz and Ostaszewski, Mateusz},
doi = {10.3390/RS12162653},
eprint = {1909.05507},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Masarczyk et al._2020_Effective training of deep convolutional neural networks for hyperspectral image classification through artificial.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Convolutional neural networks,Deep learning,Hyperspectral image classification,Transfer learning,Unsupervised training sample selection},
month = {aug},
number = {16},
pages = {2653},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Effective training of deep convolutional neural networks for hyperspectral image classification through artificial labeling}},
url = {https://www.mdpi.com/2072-4292/12/16/2653/htm https://www.mdpi.com/2072-4292/12/16/2653},
volume = {12},
year = {2020}
}
@misc{Johansen2020,
abstract = {Skin cancer is one of the most common types of cancer. Skin cancers are classified as nonmelanoma and melanoma, with the first type being the most frequent and the second type being the most deadly. The key to effective treatment of skin cancer is early detection. With the recent increase of computational power, the number of algorithms to detect and classify skin lesions has increased. The overall verdict on systems based on clinical and dermoscopic images captured with conventional RGB (red, green, and blue) cameras is that they do not outperform dermatologists. Computer-based systems based on conventional RGB images seem to have reached an upper limit in their performance, while emerging technologies such as hyperspectral and multispectral imaging might possibly improve the results. These types of images can explore spectral regions beyond the human eye capabilities. Feature selection and dimensionality reduction are crucial parts of extracting salient information from this type of data. It is necessary to extend current classification methodologies to use all of the spatiospectral information, and deep learning models should be explored since they are capable of learning robust feature detectors from data. There is a lack of large, high-quality datasets of hyperspectral skin lesion images, and there is a need for tools that can aid with monitoring the evolution of skin lesions over time. To understand the rich information contained in hyperspectral images, further research using data science and statistical methodologies, such as functional data analysis, scale-space theory, machine learning, and so on, are essential. This article is categorized under: Applications of Computational Statistics > Health and Medical Data/Informatics.},
author = {Johansen, Thomas Haugland and M{\o}llersen, Kajsa and Ortega, Samuel and Fabelo, Himar and Garcia, Aday and Callico, Gustavo M. and Godtliebsen, Fred},
booktitle = {Wiley Interdisciplinary Reviews: Computational Statistics},
doi = {10.1002/wics.1465},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Johansen et al._2020_Recent advances in hyperspectral imaging for melanoma detection.pdf:pdf},
issn = {19390068},
keywords = {hyperspectral,machine learning,melanoma,skin cancer},
month = {jan},
number = {1},
pages = {e1465},
publisher = {John Wiley & Sons, Ltd},
title = {{Recent advances in hyperspectral imaging for melanoma detection}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/wics.1465 https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1465 https://wires.onlinelibrary.wiley.com/doi/10.1002/wics.1465},
volume = {12},
year = {2020}
}
@article{Yu2017,
abstract = {As a powerful visual model, convolutional neural networks (CNNs) have demonstrated remarkable performance in various visual recognition problems, and attracted considerable attention in recent years. However, due to the highly correlated bands and insufficient training samples of hyperspectral image data, it still remains a challenging problem to effectively apply the CNN models on hyperspectral images. In this paper, an efficient CNN architecture has been proposed to boost its discriminative capability for hyperspectral image classification, in which the original data is used as the input and the final CNN outputs are the predicted class-related results. The proposed CNN infrastructure has several distinct advantages. Firstly, different from traditional classification methods those need hand-crafted features, the CNN model used here is designed to deal with the problem of hyperspectral image analysis in an end-to-end way. Secondly, the parameters of the CNN model are optimized from a small training set, while the over-fitting problem of the neural network has been alleviated to some extent. Finally, in order to better deal with the hyperspectral image information, 1 × 1 convolutional layers have been adopted, and an average pooling layer and larger dropout rates have also been employed in the whole CNN procedure. The experiments on three benchmark data sets have demonstrated that the proposed CNN architecture considerably outperforms other state-of-the-art methods.},
author = {Yu, Shiqi and Jia, Sen and Xu, Chunyan},
doi = {10.1016/j.neucom.2016.09.010},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural networks,Deep learning,Hyperspectral image classification},
month = {jan},
pages = {88--98},
publisher = {Elsevier B.V.},
title = {{Convolutional neural networks for hyperspectral image classification}},
volume = {219},
year = {2017}
}
@inproceedings{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2014/Lin et al._2014_Microsoft COCO Common Objects in Context.pdf:pdf},
issn = {16113349},
month = {may},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
url = {http://arxiv.org/abs/1405.0312},
volume = {8693 LNCS},
year = {2014}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2006/Hinton, Salakhutdinov_2006_Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {00368075},
journal = {Science},
month = {jul},
number = {5786},
pages = {504--507},
pmid = {16873662},
publisher = {American Association for the Advancement of Science},
title = {{Reducing the dimensionality of data with neural networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16873662},
volume = {313},
year = {2006}
}
@article{Isola2017,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun Yan and Zhou, Tinghui and Efros, Alexei A.},
doi = {10.1109/CVPR.2017.632},
eprint = {1611.07004},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Isola et al._2017_Image-to-image translation with conditional adversarial networks.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5967--5976},
title = {{Image-to-image translation with conditional adversarial networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Groetsch2020,
abstract = {Above-surface radiance observations of water need to be corrected for reflections on the surface to derive reflectance. The three-component glint model (3C) [ Opt. Express 25 , A742 ( 2017 ) OPEXFF 1094-4087 10.1364/OE.25.0000A1 ] was developed to spectrally resolve contributions of sky and sun glint to the surface-reflected radiance signal L r ( $\lambda$ ) , and for observations recorded at high wind speed and with fixed-position measurement geometries that frequently lead to significant sun glint contributions. Performance and limitations of 3C are assessed for all relevant wind speeds, clear sky atmospheric conditions, illumination/viewing geometries, and sun glint contamination levels. For this purpose, a comprehensive set of L r ( $\lambda$ ) spectra was simulated with a spectrally resolved sky radiance distribution model and Cox–Munk wave slope statistics. Reflectances were also derived from an extensive four-year data set of continuous above-surface hyperspectral observations from the Long Island Sound Coastal Observatory, allowing to corroborate 3C processing results from simulations and measurements with regard to sky and sun glint contributions. Simulation- and measurement-derived L r ( $\lambda$ ) independently indicate that spectral dependencies of the sky light distribution and sun glint contributions may not be neglected for observations recorded at wind speeds exceeding 4 m / s , even for sun glint-minimizing measurement geometries (Sun-sensor azimuth angle $\Delta$ ϕ = 90 − 135 ° ). These findings are in accordance with current measurement protocols for satellite calibration/validation activities. In addition, it is demonstrated that 3C is able to reliably derive water reflectance for wind speeds up to 8 m/s and $\Delta$ ϕ > 20 ° .},
author = {Groetsch, Philipp M M and Foster, Robert and Gilerson, Alexander},
doi = {10.1364/ao.385853},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Groetsch, Foster, Gilerson_2020_Exploring the limits for sky and sun glint correction of hyperspectral above-surface reflectance observa.pdf:pdf},
issn = {1559-128X},
journal = {Applied Optics},
number = {9},
pages = {2942},
pmid = {32225848},
title = {{Exploring the limits for sky and sun glint correction of hyperspectral above-surface reflectance observations}},
volume = {59},
year = {2020}
}
@article{Maaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2008/Maaten, Hinton_2008_Visualizing Data using t-SNE.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Dimensionality reduction,Embedding algorithms,Manifold learning,Multidimensional scaling,Visualization},
number = {86},
pages = {2579--2625},
title = {{Visualizing data using t-SNE}},
url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
volume = {9},
year = {2008}
}
@inproceedings{Chalapathy,
abstract = {PCA is a classical statistical technique whose simplicity and maturity has seen it find widespread use for anomaly detection. However, it is limited in this regard by being sensitive to gross perturbations of the input, and by seeking a linear subspace that captures normal behaviour. The first issue has been dealt with by robust PCA, a variant of PCA that explicitly allows for some data points to be arbitrarily corrupted; however, this does not resolve the second issue, and indeed introduces the new issue that one can no longer inductively find anomalies on a test set. This paper addresses both issues in a single model, the robust autoencoder. This method learns a nonlinear subspace that captures the majority of data points, while allowing for some data to have arbitrary corruption. The model is simple to train and leverages recent advances in the optimisation of deep neural networks. Experiments on a range of real-world datasets highlight the model's effectiveness.},
archivePrefix = {arXiv},
arxivId = {1704.06743},
author = {Chalapathy, Raghavendra and Menon, Aditya Krishna and Chawla, Sanjay},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-71249-9_3},
eprint = {1704.06743},
file = {:C\:/Users/mha114/Dropbox/Litteratur/Unknown/Chalapathy, Menon, Chawla_Unknown_Robust, Deep and Inductive Anomaly Detection.pdf:pdf},
isbn = {9783319712482},
issn = {16113349},
keywords = {Anomaly detection,Autoencoders,Deep learning,Outlier detection,Robust PCA},
pages = {36--51},
title = {{Robust, Deep and Inductive Anomaly Detection}},
volume = {10534 LNAI},
year = {2017}
}
@article{He2020,
abstract = {We propose a novel method and system that utilizes a popular smartphone to realize hyperspectral imaging for analyzing skin morphological features and monitoring hemodynamics. The imaging system works based on a built-in RGB camera and flashlight on the smartphone. We apply Wiener estimation to transform the acquired RGB-mode images into “pseudo”-hyperspectral images with 16 wavebands, covering a visible range from 470nm to 620nm. The processing method uses weighted subtractions between wavebands to extract absorption information caused by specific chromophores within skin tissue, mainly including hemoglobin and melanin. Based on the extracted absorption information of hemoglobin, we conduct real-time monitoring experiments in the skin to measure heart rate and to observe skin activities during a vascular occlusion event. Compared with expensive hyperspectral imaging systems, the smartphone-based system delivers similar results but with very-high imaging resolution. Besides, it is easy to operate, very cost-effective and has a wider customer base. The use of an unmodified smartphone to realize hyperspectral imaging promises a possibility to bring a hyperspectral analysis of skin out from laboratory and clinical wards to daily life, which may also impact on healthcare in low resource settings and rural areas.},
author = {He, Qinghua and Wang, Ruikang},
doi = {10.1364/boe.378470},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/He, Wang_2020_Hyperspectral imaging enabled by an unmodified smartphone for analyzing skin morphological features and monitoring hemodyn.pdf:pdf},
issn = {2156-7085},
journal = {Biomedical Optics Express},
keywords = {Hyperspectral imaging,Imaging systems,Imaging techniques,Liquid crystal filters,Multispectral imaging,Spectral imaging},
month = {feb},
number = {2},
pages = {895},
publisher = {The Optical Society},
title = {{Hyperspectral imaging enabled by an unmodified smartphone for analyzing skin morphological features and monitoring hemodynamics}},
volume = {11},
year = {2020}
}
@article{Liang2016,
abstract = {In recent years, deep learning has been widely studied for remote sensing image analysis. In this paper, we propose a method for remotely-sensed image classification by using sparse representation of deep learning features. Specifically, we use convolutional neural networks (CNN) to extract deep features from high levels of the image data. Deep features provide high level spatial information created by hierarchical structures. Although the deep features may have high dimensionality, they lie in class-dependent sub-spaces or sub-manifolds. We investigate the characteristics of deep features by using a sparse representation classification framework. The experimental results reveal that the proposed method exploits the inherent low-dimensional structure of the deep features to provide better classification results as compared to the results obtained by widely-used feature exploration algorithms, such as the extended morphological attribute profiles (EMAPs) and sparse coding (SC).},
author = {Liang, Heming and Li, Qi},
doi = {10.3390/rs8020099},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2016/Liang, Li_2016_Hyperspectral imagery classification using sparse representations of convolutional neural network features.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Deep features,Deep learning,Remote sensing image classific,Sparse representation},
month = {jan},
number = {2},
pages = {99},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Hyperspectral imagery classification using sparse representations of convolutional neural network features}},
url = {http://www.mdpi.com/2072-4292/8/2/99},
volume = {8},
year = {2016}
}
@inproceedings{Liu2022a,
abstract = {With recent abundant availability of high resolution multi-sensor UAV data and rapid development of deep learning models, efficient automatic mapping using deep neural network is becoming a common approach. However, with the ever-expanding inventories of both data and deep neural network models, it can be confusing to know how to choose. Most models expect input as conventional RGB format, but that can be extended to incorporate multi-sensor data. In this study, we re-implement and modify three deep neural network models of various complexities, namely UNET, DeepLabv3+ and Dense Dilated Convolutions Merging Network to use both RGB and near infrared (NIR) data from a multi-sensor UAV dataset over a Norwegian coastal area. The dataset has been carefully annotated by marine experts for coastal habitats. We find that the NIR channel increases UNET performance significantly but has mixed effects on DeepLabv3+ and DDCM. The latter two are capable of achieving best performance with RGB-only. The class-wise evaluation shows that the NIR channel greatly increases the performance in UNET for green, red algae, vegetation and rock. However, the purpose of the study is not to merely compare the models or to achieve the best performance, but to gain more insights on the compatibility between various models and data types. And as there is an ongoing effort in acquiring and annotating more data, we aim to include them in the coming year.},
author = {Liu, Y. and Liu, Q. and Sample, J. E. and Hancke, K. and Salberg, A. B.},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-Annals-V-3-2022-439-2022},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2022/Liu et al._2022_COASTAL HABITAT MAPPING WITH UAV MULTI-SENSOR DATA AN EXPERIMENT AMONG DCNN-BASED APPROACHES(2).pdf:pdf},
issn = {21949050},
keywords = {UAV,data fusion,deep learning,environmental monitoring,multi-sensor data,semantic segmentation},
month = {may},
number = {3},
pages = {439--445},
title = {{Coastal habitat mapping with uav multi-sensor data: An experiment among dcnn-based approaches}},
url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/V-3-2022/439/2022/},
volume = {5},
year = {2022}
}
@inproceedings{Penatti2015,
abstract = {In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.},
author = {Penatti, Otavio A.B. and Nogueira, Keiller and {Dos Santos}, Jefersson A.},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2015.7301382},
isbn = {9781467367592},
issn = {21607516},
keywords = {Accuracy,Correlation,Feature extraction,Histograms,Image color analysis,Remote sensing,Visualization},
pages = {44--51},
title = {{Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?}},
url = {https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W13/html/Penatti_Do_Deep_Features_2015_CVPR_paper.html},
volume = {2015-Octob},
year = {2015}
}
@inproceedings{Bergmann2019,
abstract = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the ?eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the ?rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.},
author = {Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00982},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Bergmann et al._2019_MVTEC ad-A comprehensive real-world dataset for unsupervised anomaly detection.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Datasets and Evaluation,Deep Learning},
pages = {9584--9592},
title = {{MVTEC ad-A comprehensive real-world dataset for unsupervised anomaly detection}},
url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.html},
volume = {2019-June},
year = {2019}
}
@article{Bertinetto2020,
abstract = {Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modifications of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist'19.},
archivePrefix = {arXiv},
arxivId = {1912.09393},
author = {Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A.},
doi = {10.1109/CVPR42600.2020.01252},
eprint = {1912.09393},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Bertinetto et al._2020_Making better mistakes Leveraging class hierarchies with deep networks.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {12503--12512},
title = {{Making better mistakes: Leveraging class hierarchies with deep networks}},
year = {2020}
}
@article{Zhong2020,
abstract = {Unmanned aerial vehicle (UAV)-borne hyperspectral systems can acquire hyperspectral imagery with a high spatial resolution (which we refer to here as H2 imagery). As a result of the low operating cost, high flexibility, and the ability to achieve real-time data acquisition, UAV-borne hyperspectral systems have become an important data source for remote sensing based agricultural monitoring. However, precise crop classification based on UAV-borne H2 imagery is a challenging task when faced with a number of different crop classes. The traditional hyperspectral classification methods, such as the spectral-based and object-oriented classification methods, have difficulty in classifying H2 imagery, faced with the problems of salt-and-pepper (SP) noise and scale selection. In this article, the deep convolutional neural network with a conditional random field classifier (CNNCRF) framework is proposed for precise crop classification with UAV-borne H2 imagery. In the proposed method, a deep convolutional neural network (CNN) is designed to extract and fuse in-depth spectral and local spatial features, and the conditional random field (CRF) model further incorporates the spatial-contextual information to improve the problem of holes and isolated regions in the classification map. Meanwhile, virtual sample augmentation based on the hyperspectral imaging mechanism is used to lessen the issue of the limited labeled samples. To validate the results, a new dataset—the Wuhan UAV-borne hyperspectral image (WHU-Hi) dataset—has been built for precise crop classification. The experimental results obtained using the WHU-Hi dataset confirm the accuracy and visualization performance of the proposed CNNCRF classification method, which outperforms the previous methods. In addition, the WHU-Hi dataset could serve as a benchmark dataset for hyperspectral image classification studies.},
author = {Zhong, Yanfei and Hu, Xin and Luo, Chang and Wang, Xinyu and Zhao, Ji and Zhang, Liangpei},
doi = {10.1016/j.rse.2020.112012},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Conditional random fields,Convolutional neural network,Precise crop classification,UAV-borne hyperspectral imagery,WHU-Hi dataset},
month = {dec},
pages = {112012},
publisher = {Elsevier Inc.},
title = {{WHU-Hi: UAV-borne hyperspectral with high spatial resolution (H2) benchmark datasets and classifier for precise crop identification based on deep convolutional neural network with CRF}},
volume = {250},
year = {2020}
}
@article{Tabernik2019,
abstract = {Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25–30 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection.},
archivePrefix = {arXiv},
arxivId = {1903.08536},
author = {Tabernik, Domen and {\v{S}}ela, Samo and Skvar{\v{c}}, Jure and Sko{\v{c}}aj, Danijel},
doi = {10.1007/s10845-019-01476-x},
eprint = {1903.08536},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Tabernik et al._2019_Segmentation-based deep-learning approach for surface-defect detection.pdf:pdf},
issn = {15728145},
journal = {Journal of Intelligent Manufacturing},
keywords = {Computer vision,Deep learning,Industry 4.0,Quality control,Segmentation networks,Surface-defect detection,Visual inspection},
number = {3},
pages = {759--776},
publisher = {Springer New York LLC},
title = {{Segmentation-based deep-learning approach for surface-defect detection}},
volume = {31},
year = {2020}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Redmon, Farhadi_2018_YOLOv3 An Incremental Improvement.pdf:pdf},
title = {{YOLOv3: An Incremental Improvement}},
url = {https://pjreddie.com/yolo/. http://arxiv.org/abs/1804.02767},
year = {2018}
}
@incollection{Krizhevsky2012,
abstract = {Neural networks are mathematical models that use learning algorithms inspired by the brain to store information. Neural networks have been used in many applications to model the unknown relations between various parameters, based on large numbers of examples. They are successfully used in medical decision making and medical applications.},
author = {Keijsers, N. L.W.},
booktitle = {Encyclopedia of Movement Disorders, Three-Volume Set},
doi = {10.1016/B978-0-12-374105-9.00493-7},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2012/Krizhevsky, Sutskever, Hinton_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9780123741059},
issn = {0140-6736},
keywords = {Artificial intelligence,Back propagation,Machine learning,Multilayer perceptron,Neural network,Training and testing},
pages = {V2--257--V2--259},
pmid = {7491034},
title = {{Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ},
year = {2010}
}
@inproceedings{Candes2011,
abstract = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the l1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well.We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces. {\textcopyright} 2011 ACM.},
author = {Cand{\`{e}}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
booktitle = {Journal of the ACM},
doi = {10.1145/1970392.1970395},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2011/Cand{\`{e}}s et al._2011_Robust principal component analysis.pdf:pdf},
issn = {00045411},
keywords = {Duality,L1-norm minimization,Low-rank matrices,Nuclear-norm minimization,Principal components,Robustness vis-a-vis outliers,Sparsity,Video surveillance},
month = {jun},
number = {3},
pages = {37},
publisher = {ACM PUB27 New York, NY, USA},
title = {{Robust principal component analysis?}},
url = {https://dl.acm.org/doi/abs/10.1145/1970392.1970395},
volume = {58},
year = {2011}
}
@article{Chen2014,
abstract = {Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification. {\textcopyright} 2014 IEEE.},
author = {Chen, Yushi and Lin, Zhouhan and Zhao, Xing and Wang, Gang and Gu, Yanfeng},
doi = {10.1109/JSTARS.2014.2329330},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2014/Chen et al._2014_Deep Learning-Based Classification of Hyperspectral Data.pdf:pdf},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Autoencoder (AE),deep learning,feature extraction,hyperspectral data classification,logistic regression,stacked autoencoder (SAE),support vector machine (SVM)},
month = {jun},
number = {6},
pages = {2094--2107},
title = {{Deep learning-based classification of hyperspectral data}},
url = {http://ieeexplore.ieee.org/document/6844831/},
volume = {7},
year = {2014}
}
@inproceedings{Zhou2017,
address = {Halifax},
author = {Zhou, Chong and Paffenroth, Randy C.},
doi = {10.1145/3097983.3098052},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Zhou, Paffenroth_2017_Anomaly Detection with Robust Deep Autoencoders.pdf:pdf},
isbn = {9781450348874},
keywords = {anomaly detection,autoencoder,autoencoders,denoising,group robust deep,robust deep autoencoders},
pages = {665--674},
title = {{Anomaly Detection with Robust Deep Autoencoders}},
year = {2017}
}
@inproceedings{Bergmann2018,
abstract = {Convolutional autoencoders have emerged as popular methods for unsupervised defect segmentation on image data. Most commonly, this task is performed by thresholding a per-pixel reconstruction error based on an `p-distance. This procedure, however, leads to large residuals whenever the reconstruction includes slight localization inaccuracies around edges. It also fails to reveal defective regions that have been visually altered when intensity values stay roughly consistent. We show that these problems prevent these approaches from being applied to complex real-world scenarios and that they cannot be easily avoided by employing more elaborate architectures such as variational or feature matching autoencoders. We propose to use a perceptual loss function based on structural similarity that examines inter-dependencies between local image regions, taking into account luminance, contrast, and structural information, instead of simply comparing single pixel values. It achieves significant performance gains on a challenging real-world dataset of nanofibrous materials and a novel dataset of two woven fabrics over state-of-the-art approaches for unsupervised defect segmentation that use per-pixel reconstruction error metrics.},
archivePrefix = {arXiv},
arxivId = {1807.02011},
author = {Bergmann, Paul and L{\"{o}}we, Sindy and Fauser, Michael and Sattlegger, David and Steger, Carsten},
booktitle = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
doi = {10.5220/0007364503720380},
eprint = {1807.02011},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Bergmann et al._2018_Improving Unsupervised Defect Segmentation by Applying Structural Similarity to Autoencoders.pdf:pdf},
isbn = {9789897583544},
keywords = {Anomaly Detection,Defect Segmentation,Unsupervised Learning},
month = {jul},
pages = {372--380},
title = {{Improving unsupervised defect segmentation by applying structural similarity to autoencoders}},
url = {http://arxiv.org/abs/1807.02011 http://dx.doi.org/10.5220/0007364503720380},
volume = {5},
year = {2019}
}
@article{Gauci2016,
abstract = {Analysis of the seabed composition over a large spatial scale is an interesting yet very challenging task. Apart from the field work involved, hours of video footage captured by cameras mounted on Remote Operated Vehicles (ROVs) have to be reviewed by an expert in order to classify the seabed topology and to identify potential anthropogenic impacts on sensitive benthic assemblages. Apart from being time consuming, such work is highly subjective and through visual inspection alone, a quantitative analysis is highly unlikely to be made. This study investigates the applicability of various Machine Learning techniques for the automatic classification of the seabed into maerl and sand regions from recorded ROV footage. ROV data collected from depths ranging between 50 m and 140 m and at 9.5 km from the northeast coastline of the Maltese Islands, is processed. Through the application of the presented technique, 5.23 GB of data corresponding to 2 h and 24 min of footage which was collected during June 2013, was initially cleaned and classified. An estimate for the percentage cover of the two benthic habitats (sandy seabed and maerl) was also computed by using artifacts encountered during the ROV survey and of known dimensions as a reference. Unlike other automatic seabed mapping techniques, the presented prototype processes video footage captured by a down-facing camera and not through acoustic backscatter. Image data is easier and much cheaper to capture. Promising results that indicate a very good degree of agreement between the true and predicted habitat type distribution values, were obtained.},
author = {Gauci, Adam and Deidun, Alan and Abela, John and {Zarb Adami}, Kristian},
doi = {10.1016/j.jart.2016.08.003},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2016/Gauci et al._2016_Machine Learning for benthic sand and maerl classification and coverage estimation in coastal areas around the Maltese.pdf:pdf},
issn = {24486736},
journal = {Journal of Applied Research and Technology},
keywords = {Decision trees,Image processing,Machine Learning,Maerl detection,Sand detection,Seabed classification},
month = {oct},
number = {5},
pages = {338--344},
publisher = {Universidad Nacional Aut{\'{o}}noma de M{\'{e}}xico, Instituto de Ciencias Aplicadas y Tecnolog{\'{i}}a},
title = {{Machine Learning for benthic sand and maerl classification and coverage estimation in coastal areas around the Maltese Islands}},
url = {http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1665-64232016000500338&lng=es&nrm=iso&tlng=en http://www.scielo.org.mx/scielo.php?script=sci_abstract&pid=S1665-64232016000500338&lng=es&nrm=iso&tlng=en},
volume = {14},
year = {2016}
}
@article{Bunting2006a,
abstract = {In mixed-species forests of complex structure, the delineation of tree crowns is problematic because of their varying dimensions and reflectance characteristics, the existence of several layers of canopy (including understorey), and shadowing within and between crowns. To overcome this problem, an algorithm for delineating tree crowns has been developed using eCognition Expert and hyperspectral Compact Airborne Spectrographic Imager (CASI-2) data acquired over a forested landscape near Injune, central east Queensland, Australia. The algorithm has six components: 1) the differentiation of forest, non-forest and understorey; 2) initial segmentation of the forest area and allocation of segments (objects) to larger objects associated with forest spectral types (FSTs); 3) initial identification of object maxima as seeds within these larger objects and their expansion to the edges of crowns or clusters of crowns; 4) subsequent classification-based separation of the resulting objects into crown or cluster classes; 5) further iterative splitting of the cluster classes to delineate more crowns; and 6) identification and subsequent merging of oversplit objects into crowns or clusters. In forests with a high density of individuals (e.g., regrowth), objects associated with tree clusters rather than crowns are delineated and local maxima counted to approximate density. With reference to field data, the delineation process provided accuracies > ∼70% (range 48-88%) for individuals or clusters of trees of the same species with diameter at breast height (DBH) exceeding 10 cm (senescent and dead trees excluded), with lower accuracies associated with dense stands containing several canopy layers, as many trees were obscured from the view of the CASI sensor. Although developed using 1-m spatial resolution CASI data acquired over Australian forests, the algorithm has application elsewhere and is currently being considered for integration into the Definiens product portfolio for use by the wider community. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Bunting, Peter and Lucas, Richard},
doi = {10.1016/j.rse.2005.12.015},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2006/Bunting, Lucas_2006_The delineation of tree crowns in Australian mixed species forests using hyperspectral Compact Airborne Spectrograph.pdf:pdf},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Australia,CASI,Classification,Crown delineation,Forests,Hyperspectral,Image segmentation,Queensland,eCognition},
number = {2},
pages = {230--248},
title = {{The delineation of tree crowns in Australian mixed species forests using hyperspectral Compact Airborne Spectrographic Imager (CASI) data}},
volume = {101},
year = {2006}
}
@inproceedings{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
doi = {10.61603/ceas.v2i1.33},
eprint = {1312.6114},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2013/Kingma, Welling_2013_Auto-Encoding Variational Bayes.pdf:pdf},
month = {dec},
title = {{Auto-encoding variational bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@article{Bradley2018,
abstract = {In this paper, we investigate the potential of unsupervised feature selection techniques for classification tasks, where only sparse training data are available. This is motivated by the fact that unsupervised feature selection techniques combine the advantages of standard dimensionality reduction techniques (which only rely on the given feature vectors and not on the corresponding labels) and supervised feature selection techniques (which retain a subset of the original set of features). Thus, feature selection becomes independent of the given classification task and, consequently, a subset of generally versatile features is retained. We present different techniques relying on the topology of the given sparse training data. Thereby, the topology is described with an ultrametricity index. For the latter, we take into account the Murtagh Ultrametricity Index (MUI) which is defined on the basis of triangles within the given data and the Topological Ultrametricity Index (TUI) which is defined on the basis of a specific graph structure. In a case study addressing the classification of high-dimensional hyperspectral data based on sparse training data, we demonstrate the performance of the proposed unsupervised feature selection techniques in comparison to standard dimensionality reduction and supervised feature selection techniques on four commonly used benchmark datasets. The achieved classification results reveal that involving supervised feature selection techniques leads to similar classification results as involving unsupervised feature selection techniques, while the latter perform feature selection independently from the given classification task and thus deliver generally versatile features.},
author = {Bradley, Patrick Erik and Keller, Sina and Weinmann, Martin},
doi = {10.3390/rs10101564},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Bradley, Keller, Weinmann_2018_Unsupervised feature selection based on ultrametricity and sparse training data A case study for the clas.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {AVIRIS data,Classification,EnMAP data,Hyperspectral imagery,Land cover,Land use,ROSIS data,Sparse training data,Ultrametricity,Unsupervised feature selection},
month = {sep},
number = {10},
pages = {1564},
publisher = {MDPI AG},
title = {{Unsupervised feature selection based on ultrametricity and sparse training data: A case study for the classification of high-dimensional hyperspectral data}},
url = {http://www.mdpi.com/2072-4292/10/10/1564},
volume = {10},
year = {2018}
}
@article{McKenzie2022,
abstract = {Seagrass meadows are a key ecosystem of the Great Barrier Reef World Heritage Area, providing one of the natural heritage attributes underpinning the reef's outstanding universal value. We reviewed approaches employed to date to create maps of seagrass meadows in the optically complex waters of the Great Barrier Reef and explored enhanced mapping approaches with a focus on emerging technologies, and key considerations for future mapping. Our review showed that field-based mapping of seagrass has traditionally been the most common approach in the GBR-WHA, with few attempts to adopt remote sensing approaches and emerging technologies. Using a series of case studies to harness the power of machine-and deep-learning, we mapped seagrass cover with PlanetScope and UAV-captured imagery in a variety of settings. Using a machine-learn-ing pixel-based classification coupled with a bootstrapping process, we were able to significantly improve maps of seagrass, particularly in low cover, fragmented and complex habitats. We also used deep-learning models to derive enhanced maps from UAV imagery. Combined, these lessons and emerging technologies show that more accurate and efficient seagrass mapping approaches are possible, producing maps of higher confidence for users and enabling the upscaling of seagrass mapping into the future.},
author = {McKenzie, Len J. and Langlois, Lucas A. and Roelfsema, Chris M.},
doi = {10.3390/rs14112604},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2022/McKenzie, Langlois, Roelfsema_2022_Improving Approaches to Mapping Seagrass within the Great Barrier Reef From Field to Spaceborne Earth.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Great Barrier Reef,UAV,deep-learning,earth observing,machine-learning,map confidence,mapping,seagrass,spaceborne},
month = {may},
number = {11},
pages = {2604},
title = {{Improving Approaches to Mapping Seagrass within the Great Barrier Reef: From Field to Spaceborne Earth Observation}},
url = {https://www.mdpi.com/2072-4292/14/11/2604},
volume = {14},
year = {2022}
}
@article{Blakey2015,
abstract = {We tested a supervised classification approach with Landsat 5 Thematic Mapper (TM) data for time-series mapping of seagrass in a subtropical lagoon. Seagrass meadows are an integral link between marine and inland ecosystems and are at risk from upstream processes such as runoff and erosion. Despite the prevalence of image-specific approaches, the classification accuracies we achieved show that pixel-based spectral classes may be generalized and applied to a time series of images that were not included in the classifier training. We employed in-situ data on seagrass abundance from 2007 to 2011 to train and validate a classification model. We created depth-invariant bands from TM bands 1, 2, and 3 to correct for variations in water column depth prior to building the classification model. In-situ data showed mean total seagrass cover remained relatively stable over the study area and period, with seagrass cover generally denser in the west than the east. Our approach achieved mapping accuracies (67% and 76% for two validation years) comparable with those attained using spectral libraries, but was simpler to implement. We produced a series of annual maps illustrating inter-annual variability in seagrass occurrence. Accuracies may be improved in future work by better addressing the spatial mismatch between pixel size of remotely sensed data and footprint of field data and by employing atmospheric correction techniques that normalize reflectances across images.},
author = {Blakey, Tara and Melesse, Assefa and Hall, Margaret O.},
doi = {10.3390/rs70505098},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Blakey, Melesse, Hall_2015_Supervised classification of benthic reflectance in shallow subtropical waters using a generalized pixel-base.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Benthic reflectance,Florida Bay,Landsat,Long-term monitoring,Seagrass landscapes,Supervised classification},
month = {apr},
number = {5},
pages = {5098--5116},
publisher = {MDPI AG},
title = {{Supervised classification of benthic reflectance in shallow subtropical waters using a generalized pixel-based classifier across a time series}},
url = {http://www.mdpi.com/2072-4292/7/5/5098},
volume = {7},
year = {2015}
}
@article{Kemker2018,
abstract = {Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.},
archivePrefix = {arXiv},
arxivId = {1703.06452},
author = {Kemker, Ronald and Salvaggio, Carl and Kanan, Christopher},
doi = {10.1016/j.isprsjprs.2018.04.014},
eprint = {1703.06452},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Kemker, Salvaggio, Kanan_2018_Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural network,Deep learning,Multispectral,Semantic segmentation,Synthetic imagery,Unmanned aerial system},
number = {April},
pages = {60--77},
publisher = {Elsevier},
title = {{Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.04.014},
volume = {145},
year = {2018}
}
@inproceedings{Makantasis2015,
abstract = {Spectral observations along the spectrum in many narrow spectral bands through hyperspectral imaging provides valuable information towards material and object recognition, which can be consider as a classification task. Most of the existing studies and research efforts are following the conventional pattern recognition paradigm, which is based on the construction of complex handcrafted features. However, it is rarely known which features are important for the problem at hand. In contrast to these approaches, we propose a deep learning based classification method that hierarchically constructs high-level features in an automated way. Our method exploits a Convolutional Neural Network to encode pixels' spectral and spatial information and a Multi-Layer Perceptron to conduct the classification task. Experimental results and quantitative validation on widely used datasets showcasing the potential of the developed approach for accurate hyperspectral data classification.},
author = {Makantasis, Konstantinos and Karantzalos, Konstantinos and Doulamis, Anastasios and Doulamis, Nikolaos},
booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
doi = {10.1109/IGARSS.2015.7326945},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Makantasis et al._2015_Deep supervised learning for hyperspectral data classification through convolutional neural networks.pdf:pdf},
isbn = {9781479979295},
keywords = {Earth observation,Imaging spectroscopy,Machine learning,Object Recognition},
month = {jul},
pages = {4959--4962},
publisher = {IEEE},
title = {{Deep supervised learning for hyperspectral data classification through convolutional neural networks}},
url = {https://ieeexplore.ieee.org/document/7326945/},
volume = {2015-Novem},
year = {2015}
}
@article{Verrelst2016,
abstract = {With current and upcoming imaging spectrometers, automated band analysis techniques are needed to enable efficient identification of most informative bands to facilitate optimized processing of spectral data into estimates of biophysical variables. This paper introduces an automated spectral band analysis tool (BAT) based on Gaussian processes regression (GPR) for the spectral analysis of vegetation properties. The GPR-BAT procedure sequentially backwards removes the least contributing band in the regression model for a given variable until only one band is kept. GPR-BAT is implemented within the framework of the free ARTMO's MLRA (machine learning regression algorithms) toolbox, which is dedicated to the transforming of optical remote sensing images into biophysical products. GPR-BAT allows (1) to identify the most informative bands in relating spectral data to a biophysical variable, and (2) to find the least number of bands that preserve optimized accurate predictions. To illustrate its utility, two hyperspectral datasets were analyzed for most informative bands: (1) a field hyperspectral dataset (400–1100 nm at 2 nm resolution: 301 bands) with leaf chlorophyll content (LCC) and green leaf area index (gLAI) collected for maize and soybean (Nebraska, US); and (2) an airborne HyMap dataset (430–2490 nm: 125 bands) with LAI and canopy water content (CWC) collected for a variety of crops (Barrax, Spain). For each of these biophysical variables, optimized retrieval accuracies can be achieved with just 4 to 9 well-identified bands, and performance was largely improved over using all bands. A PROSAIL global sensitivity analysis was run to interpret the validity of these bands. Cross-validated RCV2 (NRMSECV) accuracies for optimized GPR models were 0.79 (12.9%) for LCC, 0.94 (7.2%) for gLAI, 0.95 (6.5%) for LAI and 0.95 (7.2%) for CWC. This study concludes that a wise band selection of hyperspectral data is strictly required for optimal vegetation properties mapping.},
archivePrefix = {arXiv},
arxivId = {2012.08640},
author = {Verrelst, Jochem and Rivera, Juan Pablo and Gitelson, Anatoly and Delegido, Jesus and Moreno, Jos{\'{e}} and Camps-Valls, Gustau},
doi = {10.1016/j.jag.2016.07.016},
eprint = {2012.08640},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2016/Verrelst et al._2016_Spectral band selection for vegetation properties retrieval using Gaussian processes regression.pdf:pdf},
issn = {1872826X},
journal = {International Journal of Applied Earth Observation and Geoinformation},
keywords = {ARTMO,Band selection,Gaussian processes regression (GPR),Hyperspectral,Machine learning,Vegetation properties},
month = {oct},
pages = {554--567},
publisher = {Elsevier},
title = {{Spectral band selection for vegetation properties retrieval using Gaussian processes regression}},
url = {https://www.sciencedirect.com/science/article/pii/S0303243416301234},
volume = {52},
year = {2016}
}
@article{Gaston2018a,
abstract = {The monitoring of invasive grasses and vegetation in remote areas is challenging, costly, and on the ground sometimes dangerous. Satellite and manned aircraft surveys can assist but their use may be limited due to the ground sampling resolution or cloud cover. Straightforward and accurate surveillance methods are needed to quantify rates of grass invasion, offer appropriate vegetation tracking reports, and apply optimal control methods. This paper presents a pipeline process to detect and generate a pixel-wise segmentation of invasive grasses, using buffel grass (Cenchrus ciliaris) and spinifex (Triodia sp.) as examples. The process integrates unmanned aerial vehicles (UAVs) also commonly known as drones, high-resolution red, green, blue colour model (RGB) cameras, and a data processing approach based on machine learning algorithms. The methods are illustrated with data acquired in Cape Range National Park, Western Australia (WA), Australia, orthorectified in Agisoft Photoscan Pro, and processed in Python programming language, scikit-learn, and eXtreme Gradient Boosting (XGBoost) libraries. In total, 342,626 samples were extracted from the obtained data set and labelled into six classes. Segmentation results provided an individual detection rate of 97% for buffel grass and 96% for spinifex, with a global multiclass pixel-wise detection rate of 97%. Obtained results were robust against illumination changes, object rotation, occlusion, background cluttering, and floral density variation.},
author = {Gaston, Kevin J.},
doi = {10.3390/s18020605},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Gaston_2018_UAVs and Machine Learning Revolutionising Invasive Grass and Vegetation Surveys in Remote Arid Lands.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {Biosecurity,Buffel grass,Cenchrus ciliaris,Drones,Remote surveillance,Spinifex,Triodia sp,Unmanned aerial vehicles (UAV),Vegetation assessments,Xgboost},
month = {feb},
number = {2},
pages = {605},
publisher = {MDPI AG},
title = {{UAVs and Machine Learning Revolutionising Invasive Grass and Vegetation Surveys in Remote Arid Lands}},
url = {http://www.mdpi.com/1424-8220/18/2/605},
volume = {18},
year = {2018}
}
@article{Zhao2016,
abstract = {In this paper, we propose a spectral-spatial feature based classification (SSFC) framework that jointly uses dimension reduction and deep learning techniques for spectral and spatial feature extraction, respectively. In this framework, a balanced local discriminant embedding algorithm is proposed for spectral feature extraction from high-dimensional hyperspectral data sets. In the meantime, convolutional neural network is utilized to automatically find spatial-related features at high levels. Then, the fusion feature is extracted by stacking spectral and spatial features together. Finally, the multiple-feature-based classifier is trained for image classification. Experimental results on well-known hyperspectral data sets show that the proposed SSFC method outperforms other commonly used methods for hyperspectral image classification.},
author = {Zhao, Wenzhi and Du, Shihong},
doi = {10.1109/TGRS.2016.2543748},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2016/Zhao, Du_2016_Spectral–Spatial Feature Extraction for Hyperspectral Image Classification A Dimension Reduction and Deep Learning Approac.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Balanced local discriminant embedding (BLDE),Feature extraction,convolutional neural network (CNN),deep learning (DL),dimension reduction (DR)},
month = {aug},
number = {8},
pages = {4544--4554},
title = {{Spectral-Spatial Feature Extraction for Hyperspectral Image Classification: A Dimension Reduction and Deep Learning Approach}},
url = {http://ieeexplore.ieee.org/document/7450160/},
volume = {54},
year = {2016}
}
@article{Carrera2017,
abstract = {Nanoproducts represent a potential growing sector and nanofibrous materials are widely requested in industrial, medical, and environmental applications. Unfortunately, the production processes at the nanoscale are difficult to control and nanoproducts often exhibit localized defects that impair their functional properties. Therefore, defect detection is a particularly important feature in smart-manufacturing systems to raise alerts as soon as defects exceed a given tolerance level and to design production processes that both optimize the physical properties and control the defectiveness of the produced materials. Here, we present a novel solution to detect defects in nanofibrous materials by analyzing scanning electron microscope images. We employ an algorithm that learns, during a training phase, a model yielding sparse representations of the structures that characterize correctly produced nanofiborus materials. Defects are then detected by analyzing each patch of an input image and extracting features that quantitatively assess whether the patch conforms or not to the learned model. The proposed solution has been successfully validated over 45 images acquired from samples produced by a prototype electrospinning machine. The low computational times indicate that the proposed solution can be effectively adopted in a monitoring system for industrial production.},
author = {Carrera, Diego and Manganini, Fabio and Boracchi, Giacomo and Lanzarone, Ettore},
doi = {10.1109/TII.2016.2641472},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Carrera et al._2017_Defect detection in SEM images of nanofibrous materials.pdf:pdf},
issn = {15513203},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {Defect and anomaly detection,nanofibrous materials,quality control,scanning electron microscope (SEM) images,smart manufacturing,sparse representations},
month = {apr},
number = {2},
pages = {551--561},
title = {{Defect detection in SEM images of nanofibrous materials}},
url = {http://ieeexplore.ieee.org/document/7790862/},
volume = {13},
year = {2017}
}
@article{Baumgardner2015,
abstract = {This publication includes the AVIRIS hyperspectral image data for Indian Pine Test Site 3 along with the reference data for this site including observation notes and photos for the fields within the approximately 2 mile by 2 mile area.},
author = {Baumgardner, Marion F. and Biehl, Larry L. and Landgrebe, David A.},
doi = {10.4231/R7RX991C},
journal = {Purdue University Research Repository},
keywords = {HUBzero,Purdue University,collaboration,data,data management plan,dataset,publishing data,repository,research,sharing data},
month = {sep},
number = {7},
title = {{220 band aviris hyperspectral image data set: June 12, 1992 indian pine test site 3}},
url = {https://doi.org/10.4231/R7RX991C},
volume = {10},
year = {2015}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Ronneberger, Fischer, Brox_2015_U-net Convolutional networks for biomedical image segmentation.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
publisher = {Springer, Cham},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28 http://link.springer.com/10.1007/978-3-319-24574-4_28},
volume = {9351},
year = {2015}
}
@article{Gege2016,
abstract = {Specular reflections of the sun and the sky at the water surface (sun glint, sky glint) can be of comparable intensity or even much higher as the water leaving radiance, even when observation geometry minimizes the sun glint. A number of approaches have been developed for correcting these glint signals, but none of them is working reliably under all conditions. Challenges are the determination of intensity and wavelength dependency of glint in measurements of high spatial and temporal resolution for which the unknown modulations of the water surface by waves are not sufficiently averaged. In general, the reflected radiance differs spectrally significantly from downwelling irradiance, hence their ratio is not a constant, but has a characteristic spectral signature that is mainly determined by scattering processes in the atmosphere. An analytic model has been developed that describes intensity and wavelength dependency of sun glint and sky glint with few parameters. It is implemented in the publicly available software WASI for forward simulation and inverse modeling of radiance and reflectance measurements. Results from applying the model to field measurements are presented.},
author = {Gege, Peter and Groetsch, Philipp},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2016/Gege, Groetsch_2016_A spectral model for correcting sun glint and sky glint.pdf:pdf},
journal = {Proceedings of Ocean Optics XXIII},
number = {Figure 1},
title = {{A spectral model for correcting sun glint and sky glint}},
year = {2016}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Long, Shelhamer, Darrell_2015_Fully Convolutional Networks for Semantic Segmentation(2).pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {640--651},
pmid = {27244717},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {39},
year = {2017}
}
@inproceedings{Ruff2018,
abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method-Deep Support Vector Data Description-, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark dataseis as well as on the detection of adversarial examples of GT-SRB stop signs.},
author = {Ruff, Lukas and Vandermeulen, Robert A. and G{\"{o}}rnitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and M{\"{u}}ller, Emmanuel and Kloft, Marius},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Ruff et al._2018_Deep One-Class Classification.pdf:pdf},
isbn = {9781510867963},
issn = {1938-7228},
month = {jul},
pages = {6981--6996},
title = {{Deep one-class classification}},
url = {http://proceedings.mlr.press/v80/ruff18a.html},
volume = {10},
year = {2018}
}
@article{Su2008,
abstract = {Textural and local spatial statistical information is important in the classification of urban areas using very high resolution imagery. This paper describes the utility of textural and local spatial statistics for the improvement of object-oriented classification for QuickBird imagery. All textural/spatial bands were used as additional bands in the supervised object-oriented classification. The texture analysis is based on two levels: segmented image objects and moving windows across the whole image. In the texture analysis over image objects, the angular second moment textural feature at a 45° angle showed an improved classification performance with regard to buildings, depicting the patterns of buildings better than any other directions. The texture analysis based on moving windows across the whole image was conducted with various window sizes (from 3 × 3 to 13 × 13), and four grey-level co-occurrence matrix (GLCM) textural features (homogeneity, contrast, angular second moment, and entropy) were calculated. The contrast feature with the 7 × 7 window size improved classification up to 6%. One type of local spatial statistics, Moran's I feature with the vertical neighbourhood rule, improved the classification accuracy even further, up to 7%. Comparison of results between spectral and spectral+textural/spatial information indicated that textural and spatial information can be used to improve the object-oriented classification of urban areas using very high resolution imagery.},
author = {Su, Wei and Li, Jing and Chen, Yunhao and Liu, Zhigang and Zhang, Jinshui and Low, Tsuey Miin and Suppiah, Inbaraj and Hashim, Siti Atikah Mohamed},
doi = {10.1080/01431160701469016},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2008/Su et al._2008_Textural and local spatial statistics for the object-oriented classification of urban areas using high resolution imagery.pdf:pdf},
issn = {13665901},
journal = {International Journal of Remote Sensing},
number = {11},
pages = {3105--3117},
title = {{Textural and local spatial statistics for the object-oriented classification of urban areas using high resolution imagery}},
volume = {29},
year = {2008}
}
@article{Ulmas2020,
abstract = {The focus of this paper is using a convolutional machine learning model with a modified U-Net structure for creating land cover classification mapping based on satellite imagery. The aim of the research is to train and test convolutional models for automatic land cover mapping and to assess their usability in increasing land cover mapping accuracy and change detection. To solve these tasks, authors prepared a dataset and trained machine learning models for land cover classification and semantic segmentation from satellite images. The results were analysed on three different land classification levels. BigEarthNet satellite image archive was selected for the research as one of two main datasets. This novel and recent dataset was published in 2019 and includes Sentinel-2 satellite photos from 10 European countries made in 2017 and 2018. As a second dataset the authors composed an original set containing a Sentinel-2 image and a CORINE land cover map of Estonia. The developed classification model shows a high overall F\textsubscript{1} score of 0.749 on multiclass land cover classification with 43 possible image labels. The model also highlights noisy data in the BigEarthNet dataset, where images seem to have incorrect labels. The segmentation models offer a solution for generating automatic land cover mappings based on Sentinel-2 satellite images and show a high IoU score for land cover classes such as forests, inland waters and arable land. The models show a capability of increasing the accuracy of existing land classification maps and in land cover change detection.},
archivePrefix = {arXiv},
arxivId = {2003.02899},
author = {Ulmas, Priit and Liiv, Innar},
eprint = {2003.02899},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Ulmas, Liiv_2020_Segmentation of Satellite Imagery using U-Net Models for Land Cover Classification.pdf:pdf},
pages = {1--11},
title = {{Segmentation of Satellite Imagery using U-Net Models for Land Cover Classification}},
url = {http://arxiv.org/abs/2003.02899},
year = {2020}
}
@article{Araujo2019,
abstract = {Mathematical derivations and open-source library to compute receptive fields of convnets, enabling the mapping of extracted features to input signals.},
author = {Araujo, Andr&eacute; and Norris, Wade and Sim, Jack},
doi = {10.23915/distill.00021},
issn = {2476-0757},
journal = {Distill},
month = {nov},
number = {11},
pages = {e21},
publisher = {Distill Working Group},
title = {{Computing Receptive Fields of Convolutional Neural Networks}},
url = {https://distill.pub/2019/computing-receptive-fields},
volume = {4},
year = {2019}
}
@article{Fauvel2013,
author = {Fauvel, M. and Tarabalka, Y. and Benediktsson, J. A. and Chanussot, J. and Tilton, J. C.},
doi = {10.1109/JPROC.2012.2197589},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = {mar},
number = {3},
pages = {652--675},
title = {{Advances in Spectral-Spatial Classification of Hyperspectral Images}},
url = {http://ieeexplore.ieee.org/document/6297992/},
volume = {101},
year = {2013}
}
@article{Norris2024,
abstract = {Monitoring salt marshes with remote sensing is necessary to evaluate their state and restoration. Determining appropriate techniques for this can be overwhelming. Our study provides insight into whether a pixel- or object-based Random Forest classification approach is best for mapping vegetation in north temperate salt marshes. We used input variables from drone images (raw reflectances, vegetation indices, and textural features) acquired in June, July, and August 2021 of a salt marsh restoration and reference site in Aulac, New Brunswick, Canada. We also investigated the importance of input variables and whether using landcover classes representing areas of change was a practical way to evaluate variation in the monthly images. Our results indicated that (1) the classifiers achieved overall validation accuracies of 91.1–95.2%; (2) pixel-based classifiers outperformed object-based classifiers by 1.3–2.0%; (3) input variables extracted from the August images were more important than those extracted from the June and July images; (4) certain raw reflectances, vegetation indices, and textural features were among the most important variables; and (5) classes that changed temporally were mapped with user's and producer's validation accuracies of 86.7–100.0%. Knowledge gained during this study will inform assessments of salt marsh restoration trajectories spanning multiple years.},
author = {Norris, Gregory S. and LaRocque, Armand and Leblon, Brigitte and Barbeau, Myriam A. and Hanson, Alan R.},
doi = {10.3390/rs16061049},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2024/Norris et al._2024_Comparing Pixel- and Object-Based Approaches for Classifying Multispectral Drone Imagery of a Salt Marsh Restoration.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Bay of Fundy,Random Forest,ecological restoration,image classification,object-based image analysis,pixel-based image analysis,wetland},
month = {mar},
number = {6},
pages = {1049},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
title = {{Comparing Pixel- and Object-Based Approaches for Classifying Multispectral Drone Imagery of a Salt Marsh Restoration and Reference Site}},
url = {https://www.mdpi.com/2072-4292/16/6/1049/htm https://www.mdpi.com/2072-4292/16/6/1049},
volume = {16},
year = {2024}
}
@inproceedings{Hoffer2015,
abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
archivePrefix = {arXiv},
arxivId = {1412.6622},
author = {Hoffer, Elad and Ailon, Nir},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24261-3_7},
eprint = {1412.6622},
isbn = {9783319242606},
issn = {16113349},
keywords = {Deep learning,Metric learning,Representation learning},
month = {oct},
pages = {84--92},
publisher = {Springer, Cham},
title = {{Deep metric learning using triplet network}},
url = {http://link.springer.com/10.1007/978-3-319-24261-3_7},
volume = {9370},
year = {2015}
}
@article{Everingham2015,
abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
author = {Everingham, Mark and Eslami, S. M.Ali and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-014-0733-5},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Everingham et al._2015_The PASCAL Visual Object Classes Challenge A Retrospective.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition,Segmentation},
number = {1},
pages = {98--136},
title = {{The Pascal Visual Object Classes Challenge: A Retrospective}},
url = {http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf},
volume = {111},
year = {2015}
}
@article{Lam2018,
abstract = {We introduce a new large-scale dataset for the advancement of object detection techniques and overhead object detection research. This satellite imagery dataset enables research progress pertaining to four key computer vision frontiers. We utilize a novel process for geospatial category detection and bounding box annotation with three stages of quality control. Our data is collected from WorldView-3 satellites at 0.3m ground sample distance, providing higher resolution imagery than most public satellite imagery datasets. We compare xView to other object detection datasets in both natural and overhead imagery domains and then provide a baseline analysis using the Single Shot MultiBox Detector. xView is one of the largest and most diverse publicly available object-detection datasets to date, with over 1 million objects across 60 classes in over 1,400 km^2 of imagery.},
archivePrefix = {arXiv},
arxivId = {1802.07856},
author = {Lam, Darius and Kuzma, Richard and McGee, Kevin and Dooley, Samuel and Laielli, Michael and Klaric, Matthew and Bulatov, Yaroslav and McCord, Brendan},
eprint = {1802.07856},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Lam et al._2018_xView Objects in Context in Overhead Imagery.pdf:pdf},
month = {feb},
title = {{xView: Objects in Context in Overhead Imagery}},
url = {http://arxiv.org/abs/1802.07856},
year = {2018}
}
@article{Blaschke2010,
abstract = {Remote sensing imagery needs to be converted into tangible information which can be utilised in conjunction with other data sets, often within widely used Geographic Information Systems (GIS). As long as pixel sizes remained typically coarser than, or at the best, similar in size to the objects of interest, emphasis was placed on per-pixel analysis, or even sub-pixel analysis for this conversion, but with increasing spatial resolutions alternative paths have been followed, aimed at deriving objects that are made up of several pixels. This paper gives an overview of the development of object based methods, which aim to delineate readily usable objects from imagery while at the same time combining image processing and GIS functionalities in order to utilize spectral and contextual information in an integrative way. The most common approach used for building objects is image segmentation, which dates back to the 1970s. Around the year 2000 GIS and image processing started to grow together rapidly through object based image analysis (OBIA - or GEOBIA for geospatial object based image analysis). In contrast to typical Landsat resolutions, high resolution images support several scales within their images. Through a comprehensive literature review several thousand abstracts have been screened, and more than 820 OBIA-related articles comprising 145 journal papers, 84 book chapters and nearly 600 conference papers, are analysed in detail. It becomes evident that the first years of the OBIA/GEOBIA developments were characterised by the dominance of 'grey' literature, but that the number of peer-reviewed journal articles has increased sharply over the last four to five years. The pixel paradigm is beginning to show cracks and the OBIA methods are making considerable progress towards a spatially explicit information extraction workflow, such as is required for spatial planning as well as for many monitoring programmes. {\textcopyright} 2009 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Blaschke, T.},
doi = {10.1016/j.isprsjprs.2009.06.004},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2010/Blaschke_2010_Object based image analysis for remote sensing.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {GEOBIA,GIScience,Multiscale image analysis,OBIA,Object based image analysis},
number = {1},
pages = {2--16},
publisher = {Elsevier B.V.},
title = {{Object based image analysis for remote sensing}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2009.06.004},
volume = {65},
year = {2010}
}
@inproceedings{Staar2019,
abstract = {Over the recent years Convolutional Neural Networks (CNN) have become the primary choice for many image-processing problems. Regarding industrial applications, they are hence especially interesting for automated optical quality inspection. However, with well-optimized processes is it often not possible to obtain a sufficiently large set of defective samples for CNN-based classification and the training objective shifts from defect classification to anomaly detection. Here we approach this problem with deep metric learning using triplet networks. Our evaluation shows promising results that even translate to novel surface/defect classes, which were not part of the training data.},
author = {Staar, Benjamin and L{\"{u}}tjen, Michael and Freitag, Michael},
booktitle = {Procedia CIRP},
doi = {10.1016/j.procir.2019.02.123},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Staar, L{\"{u}}tjen, Freitag_2019_Anomaly detection with convolutional neural networks for industrial surface inspection.pdf:pdf},
issn = {22128271},
keywords = {Artificial intelligence,Inspection,Neural network,Pattern recognition},
month = {jan},
pages = {484--489},
publisher = {Elsevier},
title = {{Anomaly detection with convolutional neural networks for industrial surface inspection}},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119302409},
volume = {79},
year = {2019}
}
@article{Zhong2018b,
abstract = {In recent years, with the rapid development of unmanned aerial vehicles (UAVs) and lightweight hyperspectral imaging (HSI) sensors, mini-UAV-borne hyperspectral remote sensing (HRS) systems have been developed and demonstrate great value and application potential. Compared to spaceborne and airborne HSI systems, mini-UAV-borne HSI systems come with relatively low manufacturing and running costs and have thus become a new research focus in the field of HRS.},
author = {Zhong, Yanfei and Wang, Xinyu and Xu, Yao and Wang, Shaoyu and Jia, Tianyi and Hu, Xin and Zhao, Ji and Wei, Lifei and Zhang, Liangpei},
doi = {10.1109/MGRS.2018.2867592},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Zhong et al._2018_Mini-UAV-Borne Hyperspectral Remote Sensing From Observation and Processing to Applications.pdf:pdf},
issn = {21686831},
journal = {IEEE Geoscience and Remote Sensing Magazine},
month = {dec},
number = {4},
pages = {46--62},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Mini-UAV-Borne Hyperspectral Remote Sensing: From Observation and Processing to Applications}},
volume = {6},
year = {2018}
}
@inproceedings{Chollet2017,
abstract = {We present an interpretation of Inception modules in con-volutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17, 000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2017/Chollet_2017_Xception Deep learning with depthwise separable convolutions.pdf:pdf},
isbn = {9781538604571},
pages = {1800--1807},
title = {{Xception: Deep learning with depthwise separable convolutions}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Richter,
abstract = {Robots that use learned perceptual models in the real world must be able to safely handle cases where they are forced to make decisions in scenarios that are unlike any of their training examples. However, state-of-the-art deep learning methods are known to produce erratic or unsafe predictions when faced with novel inputs. Furthermore, recent ensemble, bootstrap and dropout methods for quantifying neural network uncertainty may not efficiently provide accurate uncertainty estimates when queried with inputs that are very different from their training data. Rather than unconditionally trusting the predictions of a neural network for unpredictable real-world data, we use an autoencoder to recognize when a query is novel, and revert to a safe prior behavior. With this capability, we can deploy an autonomous deep learning system in arbitrary environments, without concern for whether it has received the appropriate training. We demonstrate our method with a vision-guided robot that can leverage its deep neural network to navigate 50% faster than a safe baseline policy in familiar types of environments, while reverting to the prior behavior in novel environments so that it can safely collect additional training data and continually improve.},
author = {Richter, Charles and Roy, Nicholas},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/rss.2017.xiii.064},
file = {:C\:/Users/mha114/Dropbox/Litteratur/Unknown/Richter, Roy_Unknown_Safe Visual Navigation via Deep Learning and Novelty Detection.pdf:pdf},
isbn = {9780992374730},
issn = {2330765X},
title = {{Safe visual navigation via deep learning and novelty detection}},
url = {http://dx.doi.org/10.15607/RSS.2017.XIII.064},
volume = {13},
year = {2017}
}
@book{Hinna2011,
author = {Hinna, Kristin Ran Choi and Rinvold, Reinert A. and Gustavsen, Trond St{\o}len},
isbn = {978-82-7634-890-3},
publisher = {H{\o}yskoleforlaget},
title = {{QED 5-10}},
year = {2011}
}
@inproceedings{Viola,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, Paul and Jones, Michael},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/cvpr.2001.990517},
isbn = {0-7695-1272-0},
issn = {10636919},
pages = {I--511--I--518},
publisher = {IEEE Comput. Soc},
title = {{Rapid object detection using a boosted cascade of simple features}},
url = {http://ieeexplore.ieee.org/document/990517/},
volume = {1},
year = {2001}
}
@article{Audebert2019,
abstract = {In recent years, deep-learning techniques revolutionized the way remote sensing data are processed. The classification of hyperspectral data is no exception to the rule, but it has intrinsic specificities that make the application of deep learning less straightforward than with other optical data. This article presents the state of the art of previous machine-learning approaches, reviews the various deeplearning approaches currently proposed for hyperspectral classification, and identifies the problems and difficulties that arise in the implementation of deep neural networks for this task. In particular, the issues of spatial and spectral resolution, data volume, and transfer of models from multimedia images to hyperspectral data are addressed. Additionally, a comparative study of various families of network architectures is provided, and a software toolbox is publicly released to allow experimenting with these methods (https://github.com/nshaud/DeepHyperX). This article is intended for both data scientists with interest in hyperspectral data and remote sensing experts eager to apply deeplearning techniques to their own data set.},
archivePrefix = {arXiv},
arxivId = {1904.10674},
author = {Audebert, Nicolas and {Le Saux}, Bertrand and Lefevre, Sebastien},
doi = {10.1109/MGRS.2019.2912563},
eprint = {1904.10674},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Audebert, Le Saux, Lefevre_2019_Deep learning for classification of hyperspectral data A comparative review.pdf:pdf},
issn = {21686831},
journal = {IEEE Geoscience and Remote Sensing Magazine},
month = {jun},
number = {2},
pages = {159--173},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deep learning for classification of hyperspectral data: A comparative review}},
volume = {7},
year = {2019}
}
@misc{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/LeCun, Bengio, Hinton_2015_Deep learning.pdf:pdf},
issn = {14764687},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@inproceedings{Donoser2008,
abstract = {In this paper we propose an efficient unsupervised texture segmentation method. We introduce a texture extension of a state-of-the-art color segmentation algorithm. We show how to use covariance matrices of low level features for texture description. These features are efficiently calculated using integral images. Furthermore, a multi-scale extension allows to provide accurate texture segmentation results. An experimental evaluation on a synthetic texture database and images of the Berkeley image database demonstrate the improved performance of the algorithm. {\textcopyright} 2008 IEEE.},
author = {Donoser, Michael and Bischof, Horst},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/icpr.2008.4761350},
isbn = {9781424421756},
issn = {10514651},
month = {dec},
pages = {1--4},
publisher = {IEEE},
title = {{Using covariance matrices for unsupervised texture segmentation}},
url = {http://ieeexplore.ieee.org/document/4761350/},
year = {2008}
}
@inproceedings{Boracchi2014,
abstract = {We address the problem of automatically detecting anomalies in images, i.e., patterns that do not conform to those appearing in a reference training set. This is a very important feature for enabling an intelligent system to autonomously check the validity of acquired data, thus performing a preliminary, automatic, diagnosis. We approach this problem in a patch-wise manner, by learning a model to represent patches belonging to a training set of normal images. Here, we consider a model based on sparse representations, and we show that jointly monitoring the sparsity and the reconstruction error of such representation substantially improves the detection performance with respect to other approaches leveraging sparse models. As an illustrative application, we consider the detection of anomalies in scanning electron microscope (SEM) images, which is essential for supervising the production of nanofibrous materials.},
author = {Boracchi, Giacomo and Carrera, Diego and Wohlberg, Brendt},
booktitle = {IEEE SSCI 2014 - 2014 IEEE Symposium Series on Computational Intelligence - IES 2014: 2014 IEEE Symposium on Intelligent Embedded Systems, Proceedings},
doi = {10.1109/INTELES.2014.7008985},
isbn = {9781479944866},
month = {dec},
pages = {47--54},
publisher = {IEEE},
title = {{Novelty detection in images by sparse representations}},
url = {http://ieeexplore.ieee.org/document/7008985/},
year = {2014}
}
