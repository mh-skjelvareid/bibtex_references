Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mundhenk,
abstract = {3D action recognition – analysis of human actions based on 3D skeleton data – becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains con-currently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.07043v1},
author = {Betina, V.},
doi = {10.1007/978-3-319-46487-9},
eprint = {arXiv:1607.07043v1},
file = {:C\:/Users/mha114/Dropbox/Litteratur/Unknown/Mundhenk et al._Unknown_A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning.pdf:pdf},
isbn = {9783319464879},
issn = {00282685},
journal = {Neoplasma},
keywords = {colorization},
number = {1},
pages = {23--32},
pmid = {5815023},
title = {{(Improved PPF)Going Further with Point Pair Features}},
url = {http://gdo-datasci.ucllnl.org/cowc/ http://link.springer.com/10.1007/978-3-319-46487-9},
volume = {16},
year = {2016}
}
@article{VanEtten2018,
abstract = {Foundational mapping remains a challenge in many parts of the world, particularly in dynamic scenarios such as natural disasters when timely updates are critical. Updating maps is currently a highly manual process requiring a large number of human labelers to either create features or rigorously validate automated outputs. We propose that the frequent revisits of earth imaging satellite constellations may accelerate existing efforts to quickly update foundational maps when combined with advanced machine learning techniques. Accordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and NVIDIA), released a large corpus of labeled satellite imagery on Amazon Web Services (AWS) called SpaceNet. The SpaceNet partners also launched a series of public prize competitions to encourage improvement of remote sensing machine learning algorithms. The first two of these competitions focused on automated building footprint extraction, and the most recent challenge focused on road network extraction. In this paper we discuss the SpaceNet imagery, labels, evaluation metrics, prize challenge results to date, and future plans for the SpaceNet challenge series.},
archivePrefix = {arXiv},
arxivId = {1807.01232},
author = {{Van Etten}, Adam and Lindenbaum, Dave and Bacastow, Todd M.},
eprint = {1807.01232},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Van Etten, Lindenbaum, Bacastow_2018_SpaceNet A Remote Sensing Dataset and Challenge Series.pdf:pdf},
month = {jul},
title = {{SpaceNet: A Remote Sensing Dataset and Challenge Series}},
url = {http://arxiv.org/abs/1807.01232},
year = {2018}
}
@inproceedings{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2014/Lin et al._2014_Microsoft COCO Common Objects in Context.pdf:pdf},
issn = {16113349},
month = {may},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
url = {http://arxiv.org/abs/1405.0312},
volume = {8693 LNCS},
year = {2014}
}
@inproceedings{Bergmann2019,
abstract = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the ?eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the ?rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.},
author = {Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00982},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2019/Bergmann et al._2019_MVTEC ad-A comprehensive real-world dataset for unsupervised anomaly detection.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Datasets and Evaluation,Deep Learning},
pages = {9584--9592},
title = {{MVTEC ad-A comprehensive real-world dataset for unsupervised anomaly detection}},
url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.html},
volume = {2019-June},
year = {2019}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Redmon, Farhadi_2018_YOLOv3 An Incremental Improvement.pdf:pdf},
title = {{YOLOv3: An Incremental Improvement}},
url = {https://pjreddie.com/yolo/. http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{Rashid2020,
abstract = {This paper describes a large dataset of underwater hyperspectral imagery that can be used by researchers in the domains of computer vision, machine learning, remote sensing, and coral reef ecology. We present the details of underwater data acquisition, processing and curation to create this large dataset of coral reef imagery annotated for habitat mapping. A diver-operated hyperspectral imaging system (HyperDiver) was used to survey 147 transects at 8 coral reef sites around the Caribbean island of Cura{\c{c}}ao. The underwater proximal sensing approach produced fine-scale images of the seafloor, with more than 2.2 billion points of detailed optical spectra. Of these, more than 10 million data points have been annotated for habitat descriptors or taxonomic identity with a total of 47 class labels up to genus-and species-levels. In addition to HyperDiver survey data, we also include images and annotations from traditional (color photo) quadrat surveys conducted along 23 of the 147 transects, which enables comparative reef description between two types of reef survey methods. This dataset promises benefits for efforts in classification algorithms, hyperspectral image segmentation and automated habitat mapping.},
author = {Rashid, Ahmad Rafiuddin and Chennu, Arjun},
doi = {10.3390/data5010019},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2020/Rashid, Chennu_2020_A trillion coral reef colors Deeply annotated underwater hyperspectral images for automated classification and habit.pdf:pdf},
issn = {23065729},
journal = {Data},
keywords = {Biodiversity,Classification,Coral reef,Habitat mapping,Hierarchical learning,Hyperspectral imaging,Image segmentation,Machine learning,Proximal sensing},
month = {feb},
number = {1},
pages = {19},
publisher = {MDPI AG},
title = {{A trillion coral reef colors: Deeply annotated underwater hyperspectral images for automated classification and habitat mapping}},
url = {https://www.mdpi.com/2306-5729/5/1/19},
volume = {5},
year = {2020}
}
@article{Baumgardner2015,
abstract = {This publication includes the AVIRIS hyperspectral image data for Indian Pine Test Site 3 along with the reference data for this site including observation notes and photos for the fields within the approximately 2 mile by 2 mile area.},
author = {Baumgardner, Marion F. and Biehl, Larry L. and Landgrebe, David A.},
doi = {10.4231/R7RX991C},
journal = {Purdue University Research Repository},
keywords = {HUBzero,Purdue University,collaboration,data,data management plan,dataset,publishing data,repository,research,sharing data},
month = {sep},
number = {7},
title = {{220 band aviris hyperspectral image data set: June 12, 1992 indian pine test site 3}},
url = {https://doi.org/10.4231/R7RX991C},
volume = {10},
year = {2015}
}
@article{Everingham2015,
abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
author = {Everingham, Mark and Eslami, S. M.Ali and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-014-0733-5},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2015/Everingham et al._2015_The PASCAL Visual Object Classes Challenge A Retrospective.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition,Segmentation},
number = {1},
pages = {98--136},
title = {{The Pascal Visual Object Classes Challenge: A Retrospective}},
url = {http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf},
volume = {111},
year = {2015}
}
@article{Lam2018,
abstract = {We introduce a new large-scale dataset for the advancement of object detection techniques and overhead object detection research. This satellite imagery dataset enables research progress pertaining to four key computer vision frontiers. We utilize a novel process for geospatial category detection and bounding box annotation with three stages of quality control. Our data is collected from WorldView-3 satellites at 0.3m ground sample distance, providing higher resolution imagery than most public satellite imagery datasets. We compare xView to other object detection datasets in both natural and overhead imagery domains and then provide a baseline analysis using the Single Shot MultiBox Detector. xView is one of the largest and most diverse publicly available object-detection datasets to date, with over 1 million objects across 60 classes in over 1,400 km^2 of imagery.},
archivePrefix = {arXiv},
arxivId = {1802.07856},
author = {Lam, Darius and Kuzma, Richard and McGee, Kevin and Dooley, Samuel and Laielli, Michael and Klaric, Matthew and Bulatov, Yaroslav and McCord, Brendan},
eprint = {1802.07856},
file = {:C\:/Users/mha114/Dropbox/Litteratur/2018/Lam et al._2018_xView Objects in Context in Overhead Imagery.pdf:pdf},
month = {feb},
title = {{xView: Objects in Context in Overhead Imagery}},
url = {http://arxiv.org/abs/1802.07856},
year = {2018}
}
